{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Event-driven machine learning for PdM focusing on log preprocessing\n",
    "\n",
    "An advanced data-driven predictive maintenance approach is presented in [10].\n",
    "The objective of this research work is to develop an alerting system that provides\n",
    "early notifications to aviation engineers for upcoming aircraft failures, providing\n",
    "the needed time for the maintenance actions. The aviation is a well-documented\n",
    "field, as all the maintenance and \n",
    "flight data are systematically logged. Hence,\n",
    "**event-based techniques** can leverage this special characteristic and **provide effective predictive solutions**. The **main challenge** is to **cope** with the large set of **og\n",
    "entries** that are essentially irrelevant **to the main failures**.\n",
    "In [10], the emphasis is placed on **log preprocessing**; therefore, we will refer\n",
    "to this methodology as **LPPdM**.\n",
    "\n",
    "10.Korvesis, P., Besseau, S., Vazirgiannis, M.: Predictive maintenance in aviation:\n",
    "Failure prediction from post \n",
    "ight reports. In: IEEE Int. Conf. on Data Engineering\n",
    "(ICDE). pp. 1414{1422 (2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Make the necessary imports.\n",
    "\n",
    "suppressMessages(library(CORElearn))\n",
    "suppressMessages(library(dplyr))\n",
    "suppressMessages(library(plyr))\n",
    "suppressMessages(library(data.table))\n",
    "suppressMessages(library(randomForest))\n",
    "suppressMessages(library(xgboost))\n",
    "suppressMessages(library(ggplot2))\n",
    "suppressMessages(library(grid))\n",
    "suppressMessages(library(argparser))\n",
    "suppressMessages(library(stringr))\n",
    "suppressMessages(library(keras))\n",
    "suppressMessages(library(kerasR))\n",
    "suppressMessages(library(plsdepot))\n",
    "suppressMessages(library(class))\n",
    "suppressMessages(library(FNN))\n",
    "suppressMessages(library(Rfast))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make an argument parser named p and keep there the necessary variables.\n",
    "p <- arg_parser(\"Implementation of the AIRBUS Predictor\")\n",
    "\n",
    "# Add a positional argument\n",
    "p <- add_argument(p, \"id\", help=\"experiment ID\")\n",
    "p <- add_argument(p, \"train\", help=\"training dataset\")\n",
    "p <- add_argument(p, \"test\", help=\"test dataset\")\n",
    "p <- add_argument(p, \"fet\", help=\"different types of the fault events\",default=151) #51#11  \n",
    "p <- add_argument(p, \"tet\", help=\"type of the target fault events\",default=151) #51#11\n",
    "p <- add_argument(p, \"--rre\", help=\"remove rare events\", default=TRUE)\n",
    "p <- add_argument(p, \"--rfe\", help=\"remove frequent events\", default=TRUE)\n",
    "p <- add_argument(p, \"--kofe\", help=\"keep only first event\", default=TRUE)\n",
    "p <- add_argument(p, \"--milt\", help=\"MIL as written in the text of the paper\", default=TRUE)\n",
    "p <- add_argument(p, \"--mili\", help=\"MIL as shonw in the Figure of the paper\", default=FALSE)\n",
    "p <- add_argument(p, \"--milthres\", help=\"MIL threshold to the sigmoid function for over-sampling\", default=0.4)\n",
    "p <- add_argument(p, \"--steepness\", help=\"steepness of the sigmoid function\", default=0.7)\n",
    "p <- add_argument(p, \"--midpoint\", help=\"midpoint of the sigmoid function (in days)\", default=151) #51\n",
    "p <- add_argument(p, \"--fs\", help=\"apply feature selection\", default=FALSE)\n",
    "p <- add_argument(p, \"--top\", help=\"# of features to keep in feature selection\", default=3)#we have max 10 feautures(before was 200)\n",
    "p <- add_argument(p, \"--rer\", help=\"rare events ratio of the target event frequency\", default=0.5)\n",
    "p <- add_argument(p, \"--fer\", help=\"frequent events ratio of the frequency of the most frequent event\", default=0.8)\n",
    "p <- add_argument(p, \"--milw\", help=\"MIL window size (in days)\", default=6)\n",
    "p <- add_argument(p, \"--pthres\", help=\"prediction threshold to the Risk value for a true positive episode\", default=0.5)\n",
    "p <- add_argument(p, \"--seed\", help=\"seed for RF\", default=400)\n",
    "p <- add_argument(p, \"--csv\", help=\"output for csv\", default=TRUE)\n",
    "\n",
    "#p <- add_argument(p, \"--step\", help=\"feature selection decrease step\", default=10)#5#1\n",
    "\n",
    "p <- add_argument(p, \"--spme\", help=\"export datasets for sequential pattern minning\", default=FALSE)\n",
    "\n",
    "setwd(\"C:/Program Files (x86)\")\n",
    "p <- add_argument(p, \"--java\", help=\"the java path\", default=\"./Java/jdk1.8.0_192/bin/java.exe\")\n",
    "\n",
    "setwd(\"C:/Users/petsi\")\n",
    "p <- add_argument(p, \"--python\", help=\"the python path\", default=\"./Anaconda/python.exe\")\n",
    "\n",
    "setwd(\"C:\")\n",
    "p <- add_argument(p, \"--cep\", help=\"complex event processing path\", default=\"C:/Users/petsi/Documents/ptyxiakh/my_spmrules.py\")\n",
    "p <- add_argument(p, \"--spmf\", help=\"the spmf path\", default=\"C:/Users/petsi/Documents/ptyxiakh/spmf.jar\")\n",
    "\n",
    "\n",
    "p <- add_argument(p, \"--conf\", help=\"minimum support (minsup)\", default=\"70%\")\n",
    "p <- add_argument(p, \"--minti\", help=\"minimum time interval allowed between two succesive itemsets of a sequential pattern\", default=4)\n",
    "p <- add_argument(p, \"--maxti\", help=\"maximum time interval allowed between two succesive itemsets of a sequential pattern\", default=5)\n",
    "p <- add_argument(p, \"--minwi\", help=\"minimum time interval allowed between the first itemset and the last itemset of a sequential pattern\", default=4)\n",
    "p <- add_argument(p, \"--maxwi\", help=\"maximum time interval allowed between the first itemset and the last itemset of a sequential pattern\", default=5)\n",
    "\n",
    "p <- add_argument(p, \"--minwint\", help=\"min # of days before failure to expect a warning for true positive decision\", default=1)#2  (1)\n",
    "p <- add_argument(p, \"--maxwint\", help=\"max # of days before failure to expect a warning for true positive decision\", default=30)#5 (30)\n",
    "\n",
    "\n",
    "\n",
    "#comparison with myatm\n",
    "p <- add_argument(p, \"--X\", help=\"# of segments/sub-windows\", default=5)#3#1     (6)\n",
    "p <- add_argument(p, \"--M\", help=\"segment legth (in days)\", default=2)#2#1       (5)   \n",
    "p <- add_argument(p, \"--Y\", help=\"length of the prediction window (in days)\", default=10)#2#3#2     (30)\n",
    "p <- add_argument(p, \"--Z\", help=\"length of the buffer window (in days)\", default=5)  #             (0)\n",
    "p <- add_argument(p, \"--N\", help=\"moving step (in days)\", default=10)#2#2#1                         (10)\n",
    "p <- add_argument(p, \"--step\", help=\"feature selection decrease step\", default=40)#5#1              (30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the necessary variables.\n",
    "\n",
    "argv = data.frame() #make a data frame named argv\n",
    "#if( length(commandArgs(trailingOnly = TRUE)) != 0){\n",
    "if(FALSE){\n",
    "  argv <- parse_args(p)\n",
    "} else {\n",
    "  #argv <- parse_args(p,c(1,\"C:/Users/Public/ptyxiakh/training_my_dataset2.csv\",\"C:/Users/Public/ptyxiakh/testing_my_dataset2.csv\",11,11))\n",
    "  #argv <- parse_args(p,c(1,\"C:/Users/Public/ptyxiakh/training_my_dataset_5years.csv\",\"C:/Users/Public/ptyxiakh/testing_my_dataset_5years.csv\",51,51))\n",
    "  argv <- parse_args(p,c(1,\"C:/Users/petsi/Documents/ptyxiakh/training_my_dataset_150.csv\",\n",
    "                         \"C:/Users/petsi/Documents/ptyxiakh/testing_my_dataset_150.csv\",151,151))    \n",
    "}\n",
    "\n",
    "\n",
    "#init the variables\n",
    "id = argv$id\n",
    "\n",
    "train_path=argv$train\n",
    "test_path=argv$test\n",
    "\n",
    "b_length = argv$fet\n",
    "target_event = argv$tet\n",
    "\n",
    "target_event_frequency_proportion_rare = argv$rer\n",
    "max_event_frequency_proportion_frequent = argv$fer\n",
    "\n",
    "top_features = argv$top\n",
    "\n",
    "milw = argv$milw\n",
    "F_thres = argv$milthres\n",
    "\n",
    "s = argv$steepness\n",
    "midpoint = argv$midpoint\n",
    "acceptance_threshold = argv$pthres\n",
    "\n",
    "export_spm = argv$spme\n",
    "\n",
    "max_warning_interval = argv$maxwint\n",
    "min_warning_interval = argv$minwint\n",
    "\n",
    "csv = argv$csv\n",
    "\n",
    "seed = argv$seed\n",
    "\n",
    "step=argv$step\n",
    "\n",
    "#comparison with myatm\n",
    "X = argv$X\n",
    "M = argv$M\n",
    "Y = argv$Y\n",
    "Z = argv$Z\n",
    "N = argv$N\n",
    "#milw=X*M #like OW\n",
    "milw=10\n",
    "F_thres=0\n",
    "\n",
    "midpoint=(X*M+Z+Y)#/2\n",
    "#midpoint=10\n",
    "\n",
    "acceptance_threshold = 0.7#0.7\n",
    "s=0.7#0.9 (0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading function\n",
    "**function: read_dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for reading the csv file and save it to a two column table.\n",
    "\n",
    "read_dataset <- function(path){\n",
    "  dataset = read.table(path, header = TRUE, sep = \",\", dec = \".\", comment.char = \"#\")\n",
    "  dataset[, 2]  <- as.numeric(dataset[, 2])\n",
    "  return(dataset)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read train and test set\n",
    "\n",
    "The recorded log types read from csv files.\n",
    "One csv file(at **train_path**) has the **training_set** and the other(at **test_path**) the **testing_set**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The test_set and training_set looks like:\"\n",
      "  Timestamps Event_id\n",
      "1 2016-12-31       36\n",
      "2 2016-12-31       43\n",
      "3 2016-12-31       58\n",
      "4 2016-12-31      112\n",
      "5 2016-12-31      120\n",
      "6 2016-12-31      130\n"
     ]
    }
   ],
   "source": [
    "# Reading train and test set.\n",
    "\n",
    "training_set = read_dataset(train_path)\n",
    "test_set =  read_dataset(test_path)\n",
    "\n",
    "print(\"The test_set and training_set looks like:\")\n",
    "print(head(test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Funtions for preprocessing\n",
    "\n",
    "In order to increase the effectiveness of the approach standard **preprocessing** techniques are applied: \n",
    "\n",
    "- **Rare events (more rare than the target event)**, are considered as extremely rare, hence they are removed to reduce the dimensionality of the data. \n",
    "\n",
    "- **Most frequent events** usually do not contain significant information since they correspond to issues of minor importance. A tf-idf (term-frequency - inverted document frequency) or a simple threshold-based approach can be used to remove most frequent events. \n",
    "\n",
    "- **Multiple occurrences** of the same event in the same segment can either be noise or may not provide useful information. Hence, multiple occurrences are shrank into a single one. \n",
    "\n",
    "- Events of minor importance occur and appear in every segment until their underlying cause is treated by the technical experts. Hence, the **first occurrence of events** that occur in consecutive segments is maintained. \n",
    "\n",
    "- To deal with the imbalance of the labels (given that the target event is rare) and as several events appear shortly before the occurrence of the target event, but only a small subset of them is related to the target event, the authors use **Multiple Instance Learning (MIL) bagging the events**. A single bag contains fault events of a single day. Using MIL, the data closer to the target event (a threshold is specified), are over-sampled.\n",
    "\n",
    "- A statistical **feature selection technique**, based on the distance of the fault events with the target event is applied, to filter out fault events, which are far from the target event.\n",
    "\n",
    "\n",
    "\n",
    "**1) function: preprocess**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess <- function(ds,TEST_DATA,REMOVE_RARE_EVENTS,REMOVE_FREQUENT_EVENTS,\n",
    "                       KEEP_ONLY_FIRST_OCCURENESS,MULTI_INSTANCE_LEARNING_TEXT,\n",
    "                       MULTI_INSTANCE_LEARNING_IMAGE,FEATURE_SELECTION,top_features,\n",
    "                       s,midpoint,b_length,target_event,\n",
    "                       target_event_frequency_proportion_rare,max_event_frequency_proportion_frequent,w,F_thres){\n",
    "  #print(nrow(ds))\n",
    "  #remove events that appear < n times. We consider n = (target event frequency)/2\n",
    "  if(REMOVE_RARE_EVENTS){\n",
    "    rre<-remove_rare_events(ds,target_event_frequency_proportion_rare,ret=TRUE) #function: remove_rare_events\n",
    "  }\n",
    "  #print(nrow(ds))\n",
    "  #remove events that appear > n times. We consider n = (target event frequency)/2\n",
    "  if(REMOVE_FREQUENT_EVENTS){\n",
    "    rfe<-remove_frequent_events(ds,max_event_frequency_proportion_frequent,ret=TRUE) #function: remove_frequent_events\n",
    "  }\n",
    "  print(nrow(ds))\n",
    "  #create for the dataset(ds) a dataframe keeping for each day the frequency of the fault events\n",
    "  episodes_list = compute_frequency_list(ds,b_length)\n",
    "  #return(    episodes_list)\n",
    "  if(REMOVE_RARE_EVENTS){\n",
    "    if(length(rre)>0){\n",
    "      for(i in 1:length(rre)){\n",
    "        col=paste(\"e_\",rre[i],sep = \"\")\n",
    "        episodes_list[[col]]=c(1:nrow(episodes_list))*0\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  if(REMOVE_FREQUENT_EVENTS){\n",
    "    if(length(rfe)>0){\n",
    "      for(i in 1:length(rfe)){\n",
    "        col=paste(\"e_\",rfe[i],sep = \"\")\n",
    "        episodes_list[[col]]=c(1:nrow(episodes_list))*0\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  #print((episodes_list[1,]))\n",
    "  \n",
    "  print(\"The starting episodes_list is:\")\n",
    "  #print(head((episodes_list)[[1]]))\n",
    "  print(\"------------------------------------------------------------------------------------------------------\")\n",
    "  \n",
    "  #binarize the vector\n",
    "  #for(ep_index in (1:length(episodes_list))){\n",
    "  #ep = episodes_list[[ep_index]]\n",
    "  #ep[2:(ncol(ep)-1)][ep[2:(ncol(ep)-1)] > 0] = 1\n",
    "  #episodes_list[[ep_index]] = ep\n",
    "  episodes_list[2:(ncol(episodes_list)-1)][episodes_list[2:(ncol(episodes_list)-1)] > 0] = 1\n",
    "  #print(length(episodes_list[,1]))\n",
    "  #}\n",
    "  print(\"After binirizing:\")\n",
    "  #print(head((episodes_list)[[1]]))\n",
    "  print(\"------------------------------------------------------------------------------------------------------\")  \n",
    "  \n",
    "  \n",
    "  #keep only the first occurness of event in consecutive segments\n",
    "  if(KEEP_ONLY_FIRST_OCCURENESS){\n",
    "    episodes_list <- keep_only_first_occureness(episodes_list) #function: keep_only_first_occureness\n",
    "    print(\"After first occurancing preprocess:\")\n",
    "    #print(head((episodes_list)[[1]]))\n",
    "    print(\"------------------------------------------------------------------------------------------------------\")  \n",
    "  }\n",
    "  \n",
    "  \n",
    "  \n",
    "  #multi-instance learning to increase the pattern frequency\n",
    "  if(MULTI_INSTANCE_LEARNING_TEXT){\n",
    "    episodes_list <- mil_text(w,F_thres,episodes_list,b_length) #function: mil_text\n",
    "    print(\"After MIL text (the returning list):\")\n",
    "    #print(head((episodes_list)[[1]]))\n",
    "    print(\"------------------------------------------------------------------------------------------------------\")  \n",
    "  } else if(MULTI_INSTANCE_LEARNING_IMAGE){\n",
    "    episodes_list <- mil_image(w,F_thres,episodes_list,b_length) #function: mil_image\n",
    "    print(\"After MIL image (the returning list):\")\n",
    "    #print(head((episodes_list)[[1]]))\n",
    "    print(\"------------------------------------------------------------------------------------------------------\")  \n",
    "  }\n",
    "  \n",
    "  \n",
    "  \n",
    "  return(episodes_list)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Functions for removing the less useful data\n",
    "\n",
    "**2a) function: remove_rare_events**\n",
    "\n",
    "**2b) function: remove_frequent_events**\n",
    "\n",
    "**2c) function: keep_only_first_occureness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for removing events.\n",
    "\n",
    "#Function for removing the rare events, considering target event's frequency.\n",
    "remove_rare_events <- function(ds,target_event_frequency_proportion_rare,\n",
    "                              ret=FALSE){\n",
    "  if(!csv){\n",
    "    print(\"~~~~~~~APPLYING PROPREPROCESSING: REMOVE RARE EVENTS~~~~~~~\")\n",
    "  }\n",
    "  a = table(ds$Event_id) #table that shows the total appearances per day\n",
    "\n",
    "  target_event_frequency = a[names(a)==target_event] #total appearances of target event\n",
    "\n",
    "  #find the rear events :the events that their frequency is smaller than a threashold\n",
    "  #threashold           :percentage of total appearances of target event\n",
    "  #percentage           :given by the user usually near 0.5 (target_event_frequency_proportion_rare) \n",
    "  rare_events = as.integer(names(a[a < target_event_frequency*target_event_frequency_proportion_rare])) \n",
    "  \n",
    "  print(\"The removing rare events(columns) are:\")\n",
    "  print(rare_events)  \n",
    "  print(\"------------------------------------------------------------------------------------------------------\")  \n",
    "  \n",
    "  if(ret){\n",
    "    return(rare_events)\n",
    "  }  \n",
    "    \n",
    "  return(ds[!(ds$Event_id %in% rare_events),]) #return the dataset without the rear events\n",
    "}\n",
    "\n",
    "#Function for removing the frequent events, considering the maximum frequency that observed.\n",
    "remove_frequent_events <- function(ds,max_event_frequency_proportion_frequent,\n",
    "                                  ret=FALSE){\n",
    "  if(!csv){\n",
    "    print(\"~~~~~~~APPLYING PROPREPROCESSING: REMOVE FREQUENT EVENTS~~~~~~~\")\n",
    "  }\n",
    "  a = table(ds$Event_id) #table that shows the total appearances per day\n",
    "\n",
    "  max_freq = sort(a,decreasing = TRUE)[[1]] #maximum total appearances of a target event\n",
    "\n",
    "  #find the rear events :the events that their frequency is bigger than a threashold\n",
    "  #threashold           :percentage of the maximum total appearances of a target event\n",
    "  #percentage           :given by the user usually near 0.5 (max_event_frequency_proportion_frequent)  \n",
    "  frequent_events = as.integer(names(a[a > max_freq*max_event_frequency_proportion_frequent]))\n",
    "\n",
    "  print(\"The removing frequent_events events(columns) are:\")\n",
    "  print(frequent_events)  \n",
    "  print(\"------------------------------------------------------------------------------------------------------\")\n",
    "  \n",
    "  if(ret){\n",
    "    return(frequent_events)\n",
    "  }    \n",
    "    \n",
    "  return(ds[!(ds$Event_id %in% frequent_events),])\n",
    "}\n",
    "\n",
    "#Function for keeping only first occurances of consecutively events.\n",
    "keep_only_first_occureness <- function(episodes_list){\n",
    "  if(!csv){\n",
    "    print(\"~~~~~~~APPLYING PROPREPROCESSING: KEEP ONLY FIRST OCCURENESS~~~~~~~\")\n",
    "  }\n",
    "  #for every episode in the episodes_list\n",
    "  \n",
    "  #keep the episode  \n",
    "  ep = episodes_list\n",
    "  \n",
    "  #For every segment of each episode starting from the end up to the second segment. \n",
    "  #We need to keep only the 1st occurness of consequtive events, hence starting from the end is the easy way.\n",
    "  for(i in (nrow(ep):2)){\n",
    "    #as we deal with binary vectors, to find the indeces that both vectors have \"1\" we sum them and check for \"2\"s in the result\n",
    "    matches = which((ep[i,2:length(ep)]+ep[i-1,2:length(ep)]) %in% c(2))\n",
    "    \n",
    "    #replace the 1s with 0s in the matching positions of the segment that is closer to the end of the episode\n",
    "    ep[i,2:length(ep)][c(matches)] = 0\n",
    "  }\n",
    "  #save changes to the episodes_list\n",
    "  episodes_list = ep\n",
    "  \n",
    "  return(episodes_list) #return the new episodes_list\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Functions to compute episodes list\n",
    "\n",
    "The post flight logs are partitioned in ranges\n",
    "defined by the occurrences of the fault(**episodes**) that PdM targets. These ranges are further\n",
    "partitioned into time-segments, which may correspond to a day or to a single\n",
    "usage of the equipment(a **day** for our example). The idea is that the segments that are closer to the end\n",
    "of the range may contain fault events that are potentially indicative of the main\n",
    "event. The **goal** is to **learn a function** that quantifies the **risk** of the targeted\n",
    "failure occurring in the near future, given the events that precede it. Hence, a\n",
    "**sigmoid function** is proposed, which **maps higher values to the segments that are\n",
    "closer to the target event**. The **steepness** and **shift** of the sigmoid function are\n",
    "configured to better map the expectation of the time before the target event at\n",
    "which correlated events will start occurring.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Episode explanation\n",
    "\n",
    "For the **test_set** the episodes are presented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for creating frequency vectors for each day.\n",
    "compute_frequency_list <- function(ds2years,b_length){\n",
    "  \n",
    "  #data.frame for frequencies\n",
    "  episode_df <- data.frame(Timestamps=as.Date(character()),Event_id=integer())\n",
    "  \n",
    "  #Change ds2years(table) to episode_df(data frame)    \n",
    "  \n",
    "  #iterate over every line of the original dataset\n",
    "  for(i in 1:nrow(ds2years)) {\n",
    "    #get the current row of ds2years(table of data set)\n",
    "    meas <- ds2years[i,]\n",
    "    #add it to data frame  \n",
    "    episode_df <- rbind(episode_df,data.frame(Timestamps=meas$Timestamps, Event_id=meas$Event_id))\n",
    "    \n",
    "  }\n",
    "  #group by day\n",
    "  aggr_episode_df = aggregate(episode_df[ ,2], FUN=function(x){return(x)}, by=list(as.Date(episode_df$Timestamps, \"%Y-%m-%d\")))\n",
    "  \n",
    "  #binarize the frequncy vector(function: compute_frequency_vectors)\n",
    "  frequency_day_vectors = compute_frequency_vector(aggr_episode_df,b_length)\n",
    "  \n",
    "  return(frequency_day_vectors)\n",
    "}\n",
    "\n",
    "#Convert event vectors to binary vectors\n",
    "compute_frequency_vector <- function(aggr_episode_df,b_length){\n",
    "  \n",
    "  #data frame for binary frequency vectors  \n",
    "  freq_aggr_episode_df <- data.frame(matrix(ncol = b_length+1, nrow = 0))\n",
    "  \n",
    "  #x keeps the names of the columns. |Timestamps||e_1||e_2|...|e_b_length|  \n",
    "  x <- c(c(\"Timestamps\"), c(paste(\"e_\",c(1:b_length),sep = \"\")))\n",
    "  \n",
    "  #iterate over every line(day) of the aggr_episode_df\n",
    "  for(i in 1:nrow(aggr_episode_df)) {\n",
    "    \n",
    "    #init a vector with b_length zeros\n",
    "    freq_vector = as.vector(integer(b_length))\n",
    "    \n",
    "    #get the current row of aggr_episode_df(frequency vector-data frame of data set)\n",
    "    seg <- aggr_episode_df[i,]\n",
    "    \n",
    "    #for every value(fault event) in the current line(that happened in the current day)\n",
    "    for(value in seg$x[[1]]){\n",
    "      #replace the 0 in freq_vector with 1 at \"value=fault event\" position \n",
    "      freq_vector[[value]] = length(which(seg$x[[1]] == value))\n",
    "    }\n",
    "    \n",
    "    #add a new line to the bin_aggr_epissode_df\n",
    "    #we use a matrix holding the elements of the new_data.frame as matrix is able to store variable of different data types\n",
    "    \n",
    "    date = as.Date(seg$Group.1[[1]],origin = \"1970-01-01\")\n",
    "    freq_vector=as.Date(freq_vector,origin = \"1970-01-01\")\n",
    "    new_df = data.frame(matrix(c(date, freq_vector),nrow=1,ncol=b_length+1))\n",
    "    freq_aggr_episode_df <- rbind(freq_aggr_episode_df,new_df)\n",
    "  }\n",
    "  #set column's name as x defines\n",
    "  colnames(freq_aggr_episode_df) <- x\n",
    "  \n",
    "  #set column \"Timestamps\" x to a Date: \"Y-m-d\" column  \n",
    "  freq_aggr_episode_df$Timestamps <- as.Date(freq_aggr_episode_df$Timestamps , origin=\"1970-01-01\")\n",
    "  \n",
    "  return(freq_aggr_episode_df)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3a) function: create_episodes_list**\n",
    "\n",
    "**3b) function: compute_frequency_vectors**\n",
    "\n",
    "**3c) function: compute_F**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions for computing episodes list.\n",
    "\n",
    "#Function for creating episodes list of each day's frequency vectors.\n",
    "create_episodes_list <- function(ds,target_event,b_length,s,midpoint){\n",
    "  if(!csv){\n",
    "    print(\"~~~~~~~CREATING FREQUENCY VECTORS AND BINARIZE THEM~~~~~~~\")\n",
    "  }\n",
    "  #devide in episodes\n",
    "  target_event_spotted = FALSE\n",
    "  \n",
    "  #a list with data.frames for the episodes (each episode one data.frame)\n",
    "  episodes_list = list()\n",
    "  \n",
    "  #data.frame for episodes\n",
    "  episode_df <- data.frame(Timestamps=as.POSIXct(character()),Event_id=integer())\n",
    "  \n",
    "  #iterate over every line of the original dataset\n",
    "  for(i in 1:nrow(ds)) {\n",
    "    #get the current row of the ds\n",
    "    meas <- ds[i,]\n",
    "    #if it is the target event enable the appropriate flag\n",
    "    if((meas$Event_id == target_event) || i==1 ){\n",
    "      target_event_spotted = TRUE\n",
    "    }\n",
    "    \n",
    "    #fill the episode data.frame with the events that are between two target events\n",
    "    if(meas$Event_id != target_event && target_event_spotted){\n",
    "      episode_df <- rbind(episode_df,data.frame(Timestamps=meas$Timestamps, Event_id=meas$Event_id))\n",
    "    } else if(meas$Event_id == target_event && target_event_spotted && is.data.frame(episode_df) && nrow(episode_df) != 0){\n",
    "      #a second occurness of the target event is spotted, close the episode\n",
    "      \n",
    "      #aggregate by day all the events to form the segments inside the episodes\n",
    "      aggr_episode_df = aggregate(episode_df[ ,2], FUN=function(x){return(x)}, by=list(Timeframe=cut(as.POSIXct(episode_df$Timestamps, format=\"%Y-%m-%d\"),\"day\"))) #%Y-%m-%dT%H:%M:%OSZ\n",
    "      \n",
    "      #binarize the frequncy vector\n",
    "      bin_aggr_episode_df = compute_frequency_vectors(aggr_episode_df,b_length,s,midpoint) #function: compute_frequency_vectors\n",
    "      \n",
    "      #add the episode to the episodes_list\n",
    "      episodes_list[[length(episodes_list)+1]] = bin_aggr_episode_df\n",
    "      \n",
    "      #reset episode_df to en empty data.frame\n",
    "      episode_df <- data.frame(Timestamps=as.POSIXct(character()),Event_id=integer())\n",
    "    }\n",
    "  }\n",
    "  return(episodes_list)\n",
    "}\n",
    "\n",
    "#Convert event vectors to binary vectors\n",
    "compute_frequency_vectors <- function(aggr_episode_df,b_length,s,midpoint){\n",
    "  \n",
    "  #data frame for binary frequency vectors   \n",
    "  freq_aggr_episode_df <- data.frame(matrix(ncol = b_length+2, nrow = 0))\n",
    "  \n",
    "  #x keeps the names of the columns. |Timestamps||e_1||e_2|...|Risk_F|   \n",
    "  x <- c(c(\"Timestamps\"), c(paste(\"e_\",c(1:b_length),sep = \"\")), c(\"Risk_F\"))\n",
    "  \n",
    "  \n",
    "  for(i in 1:nrow(aggr_episode_df)) {\n",
    "    \n",
    "    #init a vector with b_length zeros\n",
    "    freq_vector = as.vector(integer(b_length))\n",
    "    \n",
    "    #get the current row of aggr_episode_df(frequency vector-data frame of data set)  \n",
    "    seg <- aggr_episode_df[i,]\n",
    "    \n",
    "    #for every value(fault event) in the current line(that happened in the current day)\n",
    "    for(value in seg$x[[1]]){\n",
    "      #replace the 0 in freq_vector with 1 at \"value=fault event\" position   \n",
    "      freq_vector[[value]] = length(which(seg$x[[1]] == value))\n",
    "    }\n",
    "    #add a new line to the bin_aggr_epissode_df\n",
    "    #we use a matrix holding the elements of the new_data.frame as matrix is able to store variable of different data types\n",
    "    \n",
    "    #F = compute_F(s,midpoint,i-1,nrow(aggr_episode_df)) #compute risk F (function: compute_F)\n",
    "    F = compute_F(s,midpoint,i,nrow(aggr_episode_df)) #compute risk F (function: compute_F)  \n",
    "    date = seg$Timeframe[[1]]\n",
    "    new_df = data.frame(matrix(c(date, freq_vector,F),nrow=1,ncol=b_length+2))\n",
    "    freq_aggr_episode_df <- rbind(freq_aggr_episode_df,new_df)\n",
    "  }\n",
    "  #set column's name as x defines  \n",
    "  colnames(freq_aggr_episode_df) <- x\n",
    "  \n",
    "  return(freq_aggr_episode_df)\n",
    "}\n",
    "\n",
    "#The Risk function(sigmoid)\n",
    "compute_F <- function(s,midpoint,t,ep_length){\n",
    "  return(1/(1+exp(s*(ep_length-midpoint-t+1))))            \n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Multi Instance Learning functions\n",
    "\n",
    "**4a) function: mil_text**\n",
    "\n",
    "**4b) function: mil_image**\n",
    "\n",
    "**4c) function: mil_text_atm**\n",
    "\n",
    "**4d) function: my_mil**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function used for over_sampled the data near target_event\n",
    "mil_text <- function(milw,F_thres,episodes_list,b_length){\n",
    "  if(!csv){\n",
    "    print(\"~~~~~~~APPLYING PROPREPROCESSING: MULTI INSTANCE LEARNING~~~~~~~\")\n",
    "  }\n",
    "  window_df = data.frame(matrix(ncol = b_length+2, nrow = 0))\n",
    "  \n",
    "  #for every episode in the episodes_list\n",
    "  for(ep_index in (1:length(episodes_list))){\n",
    "    ep = episodes_list[[ep_index]]\n",
    "    new_ep = data.frame(matrix(ncol = b_length+2, nrow = 0))\n",
    "    i = 1\n",
    "    while(i <= nrow(ep)){\n",
    "      new_ep = rbind(new_ep,ep[i,])\n",
    "      if(ep[i,][b_length+2] >= F_thres && nrow(window_df) < milw){\n",
    "        window_df = rbind(window_df,ep[i,])\n",
    "      }\n",
    "      if(nrow(window_df) == milw || i == nrow(ep)){\n",
    "        mean = colMeans(window_df)\n",
    "        mean[mean > 0] = 1\n",
    "        mf = data.frame(as.list(mean))\n",
    "        mf[1] = ep[i,][1]\n",
    "        mf[b_length+2] = ep[i,][b_length+2]\n",
    "        #colnames(mf) = colnames(new_ep)\n",
    "        new_ep = rbind(new_ep,mf)\n",
    "        if(nrow(window_df) > 1){\n",
    "          i = i - (nrow(window_df)-2)\n",
    "        }\n",
    "        window_df = data.frame(matrix(ncol = b_length+2, nrow = 0))\n",
    "      }\n",
    "      i = i + 1\n",
    "    }\n",
    "    episodes_list[[ep_index]] = new_ep\n",
    "  }\n",
    "  return(episodes_list)\n",
    "}\n",
    "\n",
    "#Function used for over_sampled the data near target_event\n",
    "mil_image <- function(milw,F_thres,episodes_list,b_length){\n",
    "  if(!csv){\n",
    "    print(\"~~~~~~~APPLYING PROPREPROCESSING: MULTI INSTANCE LEARNING~~~~~~~\")\n",
    "  }\n",
    "  \n",
    "  #for every episode in the episodes_list\n",
    "  for(ep_index in (1:length(episodes_list))){\n",
    "    ep = episodes_list[[ep_index]]\n",
    "    new_ep = data.frame(matrix(ncol = b_length+2, nrow = 0))\n",
    "    #a data.frame with the vectors that need to be averaged\n",
    "    window_df = data.frame(matrix(ncol = b_length+2, nrow = 0))\n",
    "    i = 1\n",
    "    while(i <= nrow(ep)){\n",
    "      #new_ep = rbind(new_ep,ep[i,])\n",
    "      if(nrow(window_df) < milw){\n",
    "        window_df = rbind(window_df,ep[i,])\n",
    "      }\n",
    "      if(nrow(window_df) == milw || i == nrow(ep)){\n",
    "        mean = colMeans(window_df)\n",
    "        mean[mean > 0] = 1\n",
    "        mf = data.frame(as.list(mean))\n",
    "        mf[1] = ep[i,][1]\n",
    "        mf[b_length+2] = ep[i,][b_length+2]\n",
    "        #colnames(mf) = colnames(new_ep)\n",
    "        new_ep = rbind(new_ep,mf)\n",
    "        if(window_df[1,][b_length+2] >= F_thres && nrow(window_df) > 1){\n",
    "          i = i - (nrow(window_df)-1)\n",
    "        }\n",
    "        window_df = data.frame(matrix(ncol = b_length+2, nrow = 0))\n",
    "      }\n",
    "      i = i + 1\n",
    "    }\n",
    "    episodes_list[[ep_index]] = new_ep\n",
    "  }\n",
    "  return(episodes_list)\n",
    "}\n",
    "\n",
    "#merged_episodes1=mil_text_atm(milw,0,episodes_list,b_length,N=N,Z=Z)\n",
    "#Function used for over_sampled the data as atm does\n",
    "mil_text_atm <- function(milw,F_thres,episodes_list,b_length,N=1,Z=0){\n",
    "  if(!csv){\n",
    "    print(\"~~~~~~~APPLYING PROPREPROCESSING: MULTI INSTANCE LEARNING~~~~~~~\")\n",
    "  }\n",
    "  window_df = data.frame(matrix(ncol = b_length+2, nrow = 0))\n",
    "  \n",
    "  for(ep_index in (1:length(episodes_list))){\n",
    "    \n",
    "    ep = episodes_list[[ep_index]]\n",
    "    \n",
    "    new_ep = data.frame(matrix(ncol = b_length+2, nrow = 0))\n",
    "    \n",
    "    i = 1\n",
    "    \n",
    "    \n",
    "    while(i <= nrow(ep)){\n",
    "      if(i<=0){\n",
    "        break\n",
    "      }\n",
    "      \n",
    "      new_ep = rbind(new_ep,ep[i,])\n",
    "      if(ep[i,][b_length+2] >= F_thres && nrow(window_df) < milw){\n",
    "        window_df = rbind(window_df,ep[i,])\n",
    "      }\n",
    "      \n",
    "      if(nrow(window_df) == milw || i == nrow(ep)-Z){  \n",
    "        mean = colMeans(window_df)#mean of each column\n",
    "        \n",
    "        mean[mean > 0] = 1#binirize the mean of each column\n",
    "        \n",
    "        mf = data.frame(as.list(mean))#make list to frame\n",
    "        \n",
    "        mf[1] = ep[i,][1]#timestamp changed from 1 to i or row that are at the currend loop\n",
    "        mf[b_length+2] = ep[i,][b_length+2]#RISK_F changed from 1 to the risk of the current i \n",
    "        \n",
    "        new_ep = rbind(new_ep,mf)\n",
    "        \n",
    "        if(nrow(window_df) > 1){\n",
    "          #i = i - (nrow(window_df)-2)#4-(4=+2) #moving step\n",
    "          i = i - (nrow(window_df)-N) #moving step  \n",
    "        }\n",
    "        window_df = data.frame(matrix(ncol = b_length+2, nrow = 0))\n",
    "        \n",
    "      }\n",
    "      i = i + 1\n",
    "    }\n",
    "    episodes_list[[ep_index]] = new_ep\n",
    "  }\n",
    "  \n",
    "  return(episodes_list)\n",
    "}\n",
    "\n",
    "#Function used for over_sampled the data by trying to equalize the data of the 2 different classes in each episode\n",
    "#separately\n",
    "#(TRUE class: the target event will happen to the given time period)\n",
    "#(FALSE class: the target event will not happen to the given time period)\n",
    "my_mil <- function(milw,episodes_list,b_length,N,threas=0.7){\n",
    "  if(!csv){\n",
    "    print(\"~~~~~~~APPLYING PROPREPROCESSING: MULTI INSTANCE LEARNING~~~~~~~\")\n",
    "  }\n",
    "  window_df = data.frame(matrix(ncol = b_length+2, nrow = 0))\n",
    "  \n",
    "  colnames(window_df) <- colnames(episodes_list)\n",
    "  for(ep_index in (1:length(episodes_list))){\n",
    "    \n",
    "    ep = episodes_list[[ep_index]]\n",
    "    \n",
    "    #if(nrow(ep)<milw){\n",
    "    #next\n",
    "    #}\n",
    "    \n",
    "    days_of_the_episode=(length(ep[,1]))\n",
    "    if(ep_index>0){\n",
    "      \n",
    "      positive_days=length(which(ep$Risk_F>threas))\n",
    "      negative_days=length(which(ep$Risk_F<=threas))\n",
    "      \n",
    "      if(negative_days==0||positive_days==0)\n",
    "      {\n",
    "        \n",
    "        milw_pos=-1\n",
    "        milw_neg=-1\n",
    "      }\n",
    "      else{\n",
    "        if(positive_days>negative_days){\n",
    "          if(positive_days>=2*negative_days){\n",
    "            milw_neg=2\n",
    "            step_neg=1\n",
    "            milw_pos=-1\n",
    "            step_pos=-1\n",
    "            \n",
    "          }\n",
    "          else if(positive_days>1.5*negative_days){\n",
    "            milw_neg=2\n",
    "            step_neg=1\n",
    "            days_to_make=2*negative_days-1-positive_days\n",
    "            \n",
    "            milw_pos=positive_days-(days_to_make)\n",
    "            \n",
    "            step_pos=1\n",
    "          }\n",
    "          else if(positive_days>1*negative_days){\n",
    "            \n",
    "            milw_neg=floor(1/4*negative_days)\n",
    "            \n",
    "            step_neg=1\n",
    "            \n",
    "            days_to_make=negative_days+(negative_days-milw_neg)-positive_days\n",
    "            \n",
    "            milw_pos=positive_days-(days_to_make)\n",
    "            \n",
    "            step_pos=1\n",
    "            \n",
    "          }\n",
    "        }\n",
    "        else if(negative_days>positive_days){\n",
    "          if(negative_days>=2*positive_days){\n",
    "            milw_neg=-1\n",
    "            step_neg=-1\n",
    "            milw_pos=2\n",
    "            step_pos=1\n",
    "            \n",
    "          }\n",
    "          else if(negative_days>1.5*positive_days){\n",
    "            \n",
    "            step_neg=1\n",
    "            milw_pos=2\n",
    "            step_pos=1\n",
    "            \n",
    "            days_to_make=2*positive_days-1-negative_days\n",
    "            \n",
    "            milw_neg=negative_days-(days_to_make)\n",
    "            \n",
    "          }\n",
    "          else if(negative_days>1*positive_days){\n",
    "            \n",
    "            step_neg=1\n",
    "            \n",
    "            milw_pos=floor(1/4*positive_days)\n",
    "            \n",
    "            step_pos=1\n",
    "            \n",
    "            days_to_make=positive_days+(positive_days-milw_pos)-negative_days\n",
    "            \n",
    "            milw_neg=negative_days-(days_to_make)\n",
    "            \n",
    "          }\n",
    "          \n",
    "        }\n",
    "        else{\n",
    "          milw_neg=2\n",
    "          step_neg=1\n",
    "          milw_pos=2\n",
    "          step_pos=1\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    new_ep = data.frame(matrix(ncol = b_length+2, nrow = 0))\n",
    "    window_df = data.frame(matrix(ncol = b_length+2, nrow = 0))\n",
    "    \n",
    "    milw=(milw_neg)\n",
    "    \n",
    "    F_thres=0\n",
    "    Z=0\n",
    "    \n",
    "    F_thres=-100\n",
    "    finish=negative_days\n",
    "    i=1\n",
    "    count=1\n",
    "    flag=TRUE\n",
    "    while(i <= finish){\n",
    "      \n",
    "      if((count<=finish)&&(count==i)){\n",
    "        new_ep = rbind(new_ep,ep[count,])\n",
    "        count=count+1\n",
    "      }\n",
    "      else{\n",
    "        if(flag==FALSE){\n",
    "          break\n",
    "        }\n",
    "      }\n",
    "      \n",
    "      \n",
    "      if(ep[i,][b_length+2] >= F_thres && nrow(window_df) < milw && milw>0 &&flag){\n",
    "        window_df = rbind(window_df,ep[i,])\n",
    "      }\n",
    "      \n",
    "      if(nrow(window_df) == milw || i == finish && milw>0&&flag){  \n",
    "        mean = colMeans(window_df)#mean of each column\n",
    "        \n",
    "        mean[mean > 0] = 1#binirize the mean of each column\n",
    "        \n",
    "        mf = data.frame(as.list(mean))#make list to frame\n",
    "        \n",
    "        mf[1] = ep[i,][1]#timestamp changed from 1 to i or row that are at the currend loop\n",
    "        mf[b_length+2] = ep[i,][b_length+2]#RISK_F changed from 1 to the risk of the current i \n",
    "        \n",
    "        \n",
    "        new_ep = rbind(new_ep,mf)\n",
    "        \n",
    "        if(nrow(window_df) > 1){\n",
    "          i = i - (nrow(window_df)-N) #moving step  \n",
    "          if(i+milw>finish){\n",
    "            flag=FALSE\n",
    "          }\n",
    "        }\n",
    "        window_df = data.frame(matrix(ncol = b_length+2, nrow = 0))\n",
    "        \n",
    "      }\n",
    "      \n",
    "      i = i + 1\n",
    "    }\n",
    "    \n",
    "    \n",
    "    new_ep2 = data.frame(matrix(ncol = b_length+2, nrow = 0))\n",
    "    window_df = data.frame(matrix(ncol = b_length+2, nrow = 0))\n",
    "    \n",
    "    i=negative_days+1\n",
    "    finish=negative_days+positive_days\n",
    "    milw=(milw_pos)\n",
    "    F_thres=-100\n",
    "    \n",
    "    count=negative_days+1\n",
    "    flag=TRUE\n",
    "    while(i <= finish){\n",
    "      \n",
    "      if((count<=finish)&&(count==i)){\n",
    "        new_ep2 = rbind(new_ep2,ep[count,])\n",
    "        count=count+1\n",
    "      }\n",
    "      else{\n",
    "        if(flag==FALSE){\n",
    "          break\n",
    "        }\n",
    "      }\n",
    "      \n",
    "      \n",
    "      if(ep[i,][b_length+2] >= F_thres && nrow(window_df) < milw && milw>0&&flag){\n",
    "        window_df = rbind(window_df,ep[i,])\n",
    "      }\n",
    "      \n",
    "      if(nrow(window_df) == milw || i == finish  && milw>0 &&flag){   \n",
    "        \n",
    "        mean = colMeans(window_df)#mean of each column\n",
    "        \n",
    "        mean[mean > 0] = 1#binirize the mean of each column\n",
    "        \n",
    "        mf = data.frame(as.list(mean))#make list to frame\n",
    "        \n",
    "        mf[1] = ep[i,][1]#timestamp changed from 1 to i or row that are at the currend loop\n",
    "        mf[b_length+2] = ep[i,][b_length+2]#RISK_F changed from 1 to the risk of the current i \n",
    "        \n",
    "        new_ep2 = rbind(new_ep2,mf)\n",
    "        \n",
    "        if(nrow(window_df) > 1){\n",
    "          i = i - (nrow(window_df)-N) #moving step  \n",
    "          if(i+milw>finish){\n",
    "            flag=FALSE\n",
    "          }\n",
    "          \n",
    "        }\n",
    "        window_df = data.frame(matrix(ncol = b_length+2, nrow = 0))\n",
    "        \n",
    "      }\n",
    "      \n",
    "      i = i + 1\n",
    "    }\n",
    "    \n",
    "    new_ep3 = rbind(new_ep,new_ep2)\n",
    "    \n",
    "    episodes_list[[ep_index]] = new_ep3\n",
    "    \n",
    "  }\n",
    "  \n",
    "  return(order_episodes(episodes_list))\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for setting data to the right form\n",
    "\n",
    "**1) function: make_new_attributes**\n",
    "\n",
    "**2) function: my_ldply**\n",
    "\n",
    "**3) function: merging_attributes**\n",
    "\n",
    "**4) function: make_Risk_F**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_new_attributes<-function(episodes_list,N,moving_step,BW,b_length,days){\n",
    "  \n",
    "  ret_episodes=list()\n",
    "  index=1\n",
    "  for(i in 1:length(days)){\n",
    "      \n",
    "    end=days[[i]]-BW\n",
    "    \n",
    "    old_episode_df=list()\n",
    "    counter=1\n",
    "    \n",
    "    while((index+N-1)<end){\n",
    "      old_episode_df=append(old_episode_df, list(episodes_list[index:(index+N-1),]), 0)\n",
    "      counter=counter+1\n",
    "      index=index+moving_step\n",
    "    }\n",
    "    \n",
    "    if(counter>1){\n",
    "      ret_episodes=append(ret_episodes, list(old_episode_df))\n",
    "    }\n",
    "\n",
    "  }\n",
    "  return(ret_episodes)\n",
    "  \n",
    "}\n",
    "\n",
    "\n",
    "my_ldply<-function(my_episode){\n",
    "  ret_list=list()\n",
    "  for(i in 1:length(my_episode)){\n",
    "    if(length(my_episode[[i]])>0){\n",
    "      for(j in 1:length(my_episode[[i]])){\n",
    "        ret_list=append(ret_list, list(my_episode[[i]][[j]]), 0)\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  return(ret_list)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "merging_attributes<-function(my_episode,N){\n",
    "  ret_episode=list()\n",
    "  if(length(my_episode)>0){\n",
    "    for(i in 1:length(my_episode)){\n",
    "      if(length(my_episode[[i]])>0){\n",
    "        test1 = my_episode[[i]][ , !(names(my_episode[[i]]) %in% c(\"Timestamps\"))] #delete column Timestamps\n",
    "        risk_save=my_episode[[i]][N,]$Risk_F\n",
    "        test1 = test1[ , !(names(test1) %in% c(\"Risk_F\"))] #delete column Timestamps\n",
    "        #print(\"1\")\n",
    "        x <- c(c(paste(\"e_\",c(1:(N*b_length)),sep = \"\")))\n",
    "        \n",
    "        vec <- c(t(test1))\n",
    "        \n",
    "        \n",
    "        names(vec) <- x\n",
    "        \n",
    "        vec<-as.data.frame(t(vec))\n",
    "        \n",
    "        \n",
    "        vec$Risk_F<-risk_save\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        ret_episode=append(ret_episode, list(vec), 0)\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  return(ret_episode)\n",
    "  \n",
    "}\n",
    "\n",
    "\n",
    "make_Risk_F <- function(train_episodes_list,days,s,midpoint){  \n",
    "  counter=1  \n",
    "  for(i in 1:length(days)){\n",
    "    while(counter<=days[[i]]){              \n",
    "      train_episodes_list[counter,][[\"Risk_F\"]]=compute_F(s,midpoint,counter,days[[i]])\n",
    "      counter=counter+1    \n",
    "    }  \n",
    "  }\n",
    "  return(train_episodes_list)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"The removing rare events(columns) are:\"\n",
      "integer(0)\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n",
      "[1] \"The removing frequent_events events(columns) are:\"\n",
      "[1]  45 103\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n",
      "[1] 9639\n",
      "[1] \"The starting episodes_list is:\"\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n",
      "[1] \"After binirizing:\"\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n",
      "[1] \"After first occurancing preprocess:\"\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing for training set.\n",
    "#milw;midpoint;stepness;N;Z;MIN;MAX;BW\n",
    "#16;16;0.7;16;8;1;16;0\n",
    "milw=16\n",
    "midpoint=16\n",
    "s=0.7\n",
    "N=16\n",
    "moving_step=8\n",
    "min_warning_interval=1\n",
    "max_warning_interval = 16\n",
    "BW = 0\n",
    "\n",
    "\n",
    "TEST_DATA = FALSE\n",
    "REMOVE_RARE_EVENTS = argv$rre\n",
    "REMOVE_FREQUENT_EVENTS = argv$rfe\n",
    "KEEP_ONLY_FIRST_OCCURENESS = argv$kofe\n",
    "MULTI_INSTANCE_LEARNING_TEXT = argv$milt #MIL as explained in the text\n",
    "MULTI_INSTANCE_LEARNING_IMAGE = argv$mili #MIL as presented in the figure\n",
    "FEATURE_SELECTION = argv$fs\n",
    "\n",
    "MULTI_INSTANCE_LEARNING_TEXT=FALSE\n",
    "MULTI_INSTANCE_LEARNING_IMAGE=FALSE\n",
    "\n",
    "episodes_list <- preprocess(training_set,TEST_DATA,REMOVE_RARE_EVENTS,REMOVE_FREQUENT_EVENTS\n",
    "                            ,KEEP_ONLY_FIRST_OCCURENESS,MULTI_INSTANCE_LEARNING_TEXT\n",
    "                            ,MULTI_INSTANCE_LEARNING_IMAGE,FEATURE_SELECTION\n",
    "                            ,top_features,s,midpoint,b_length\n",
    "                            ,target_event,target_event_frequency_proportion_rare\n",
    "                            ,max_event_frequency_proportion_frequent,milw,F_thres)\n",
    "\n",
    "\n",
    "episodes_list[, \"Risk_F\"] <- 0\n",
    "        \n",
    "days=list()\n",
    "sum=0\n",
    "counter=1\n",
    "for(ep in 1:nrow(episodes_list)){\n",
    "    if(episodes_list[ep,][[paste(\"e_\",target_event,sep=\"\")]]==1)\n",
    "        {      \n",
    "            days[[counter]]<-ep\n",
    "            sum=sum+days[[counter]][1]\n",
    "            counter<-counter+1\n",
    "          }  \n",
    "         \n",
    "    }  \n",
    "episodes_list=make_Risk_F(episodes_list,days,s,midpoint)\n",
    "lets_try=make_new_attributes(episodes_list,N=N,moving_step =moving_step ,BW=BW,b_length,days)\n",
    "my_final=my_ldply(lets_try)\n",
    "my_episodes=merging_attributes(my_final,N=N)\n",
    "merged_episodes1 = ldply(my_episodes, data.frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing of testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 5100\n",
      "[1] \"The starting episodes_list is:\"\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n",
      "[1] \"After binirizing:\"\n",
      "[1] \"------------------------------------------------------------------------------------------------------\"\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing for testing set.\n",
    "\n",
    "TEST_DATA = TRUE\n",
    "REMOVE_RARE_EVENTS = FALSE\n",
    "REMOVE_FREQUENT_EVENTS = FALSE\n",
    "KEEP_ONLY_FIRST_OCCURENESS = FALSE\n",
    "MULTI_INSTANCE_LEARNING_TEXT = FALSE #MIL as explained in the text\n",
    "MULTI_INSTANCE_LEARNING_IMAGE = FALSE #MIL as presented in the figure\n",
    "FEATURE_SELECTION = FALSE\n",
    "\n",
    "test_episodes <- preprocess(test_set,TEST_DATA,REMOVE_RARE_EVENTS,REMOVE_FREQUENT_EVENTS,KEEP_ONLY_FIRST_OCCURENESS,MULTI_INSTANCE_LEARNING_TEXT,MULTI_INSTANCE_LEARNING_IMAGE,FEATURE_SELECTION,top_features,s,midpoint,b_length,target_event,target_event_frequency_proportion_rare,max_event_frequency_proportion_frequent,milw,F_thres)\n",
    "        \n",
    "days=list()\n",
    "counter=1\n",
    "sum=0\n",
    "for(ep in 1:nrow(test_episodes)){\n",
    "    if(test_episodes[ep,][[paste(\"e_\",target_event,sep=\"\")]]==1)\n",
    "    {     \n",
    "         days[[counter]]<-ep\n",
    "         sum=sum+days[[counter]][1]\n",
    "        counter<-counter+1\n",
    "    }  \n",
    "          \n",
    "          \n",
    "}  \n",
    "        \n",
    "test_episodes[, \"Risk_F\"] <- 0\n",
    "test_episodes=make_Risk_F(test_episodes,days,s,midpoint)\n",
    "        \n",
    "\n",
    "test_episodes_list=make_new_attributes(test_episodes,N=N,moving_step =moving_step ,BW=BW,b_length,days)\n",
    "        \n",
    "for(row in 1:length(test_episodes_list)){\n",
    "    test_episodes_list[[row]]=merging_attributes(test_episodes_list[[row]],N)\n",
    "    test_episodes_list[[row]]=ldply(test_episodes_list[[row]], data.frame)\n",
    "}\n",
    "        \n",
    "        \n",
    "for(ep in 0:(length(days)-2)){\n",
    "    days[[length(days)-ep]]<-days[[length(days)-ep]]-days[[length(days)-ep-1]]\n",
    "}\n",
    "        \n",
    "        \n",
    "for(ep in test_episodes_list){\n",
    "    ep = ep[ , !(names(ep) %in% c(\"Timestamps\"))]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Feature Selection\n",
    "**1) function: top_feature_selection**\n",
    "\n",
    "**2) function: best_feature_selection**\n",
    "\n",
    "**3) PCA functions:**\n",
    "      \n",
    "   -   **3a) remove_zero_columns**\n",
    "   \n",
    "   -   **3a) pca_function**\n",
    "   \n",
    "   -   **3a) pca_dimensionality_reduction**\n",
    "\n",
    "**4) function: feature_reduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtion for selectiong the top features using ReliefF.\n",
    "top_feature_selection <- function(merged_episodes,top_features,seed=0){\n",
    "  #Feature selection using reliefF\n",
    "  set.seed(seed)  \n",
    "  #attrEval function -> https://www.rdocumentation.org/packages/CORElearn/versions/1.53.1/topics/attrEval  \n",
    "  #ReliefF -> https://medium.com/@yashdagli98/feature-selection-using-relief-algorithms-with-python-example-3c2006e18f83      \n",
    "  estReliefF <- attrEval(Risk_F ~ ., merged_episodes, estimator=\"RReliefFexpRank\", ReliefIterations=50)\n",
    "  \n",
    "  #sort indeces of  estReliefF   \n",
    "  sorted_indeces = order(estReliefF, decreasing = TRUE)\n",
    "  \n",
    "  #keep the the top (top_features) \"useful\" columns of instances data frame  \n",
    "  merged_episodes = merged_episodes %>% select(sorted_indeces[1:top_features],ncol(merged_episodes))\n",
    "  \n",
    "  return(merged_episodes)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funtion for selectiong the best feature for the selected algorithm using ReliefF.\n",
    "best_feature_selection <- function(merged_episodes,test_episodes_list,step,choice=0,acc_thres=0.5){\n",
    "  \n",
    "  run = TRUE\n",
    "  max_F1=0 #variable for keeping the max_F1 score\n",
    "  max_instances_df = data.frame() #empty data frame named max_instances_df\n",
    "  \n",
    "  merged_episodes=((merged_episodes[colSums(merged_episodes) > 0]))\n",
    "  \n",
    "  i = length(merged_episodes)-1 #-1 for taking out the label\n",
    "  \n",
    "  while(run){\n",
    "    #Feature selection using reliefF\n",
    "    \n",
    "    #attrEval function -> https://www.rdocumentation.org/packages/CORElearn/versions/1.53.1/topics/attrEval  \n",
    "    #ReliefF -> https://medium.com/@yashdagli98/feature-selection-using-relief-algorithms-with-python-example-3c2006e18f83  \n",
    "    estReliefF <- attrEval(Risk_F ~ . , merged_episodes, estimator=\"ReliefFexpRank\", ReliefIterations=50)\n",
    "    \n",
    "    #sort indeces of  estReliefF \n",
    "    sorted_indeces = order(estReliefF, decreasing = TRUE)  \n",
    "    #print(sorted_indeces)\n",
    "    #keep the the top i \"useful\" columns of instances data frame  \n",
    "    merged_episodes = merged_episodes %>% select(sorted_indeces[1:i],ncol(merged_episodes))\n",
    "    \n",
    "    for(j in 1:length(test_episodes_list)){\n",
    "      test_episodes_list[[j]]=(test_episodes_list[[j]][,names(merged_episodes)])\n",
    "    } \n",
    "    \n",
    "    \n",
    "    #if choice==1: find F1 score using RF(function: RFfit)\n",
    "    if(choice==1){\n",
    "      my.rf = RFfit(merged_episodes,seed,FALSE)\n",
    "      F1=eval(test_episodes_list,my.rf,acc_thres)\n",
    "    }\n",
    "    #if choice==2: find F1 score using XGBoost(function: XGBoostfit)    \n",
    "    else if(choice==2){\n",
    "      my.xgb = XGBoostfit(merged_episodes,seed,FALSE)\n",
    "      F1=eval(test_episodes_list,my.xgb,acc_thres,TRUE)\n",
    "    }\n",
    "    else{\n",
    "      F1=0\n",
    "    }\n",
    "    \n",
    "    \n",
    "    #if max F1 score is 0(first iteration)  \n",
    "    if(max_F1 == 0){\n",
    "      max_F1 = F1\n",
    "      max_instances_df = merged_episodes\n",
    "    } else if(F1>max_F1){ #if F1>=max_F1(which means that with less data we have at least the same F1 score)\n",
    "      max_F1 = F1 #set as new max F1 score the current F1 score\n",
    "      max_instances_df = merged_episodes #set new max_instances_df the current instances_df\n",
    "    }\n",
    "    i = i - step #-step for taking out the least \"useful\" columns\n",
    "    if(i <= 0){\n",
    "      run = FALSE\n",
    "    }\n",
    "  }\n",
    "  return(max_instances_df)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA functions\n",
    "\n",
    "#PCA functions\n",
    "#Function for removing the zero columns of an array or a data_frame.\n",
    "remove_zero_columns <- function(data_array){\n",
    "  data_array=data.frame(data_array)\n",
    "  return((data_array[colSums(data_array) > 0]))\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Functions tha makes pca to a given array.\n",
    "pca_function <- function(flat_Xtrain_non_zero_cols){\n",
    "  \n",
    "  #for every element of the flat_Xtrain_non_zero_cols subtract the mean of its column.  \n",
    "  for(i in 1:length(flat_Xtrain_non_zero_cols[1,])){\n",
    "    flat_Xtrain_non_zero_cols[,i]=flat_Xtrain_non_zero_cols[,i]-mean(flat_Xtrain_non_zero_cols[,i])\n",
    "  }\n",
    "  \n",
    "  #Principal Components Analysis  \n",
    "  pca.out <- prcomp((flat_Xtrain_non_zero_cols),center = TRUE)\n",
    "  \n",
    "  #the matrix of variable loadings (i.e., a matrix whose columns contain the eigenvectors)    \n",
    "  u=(pca.out$rotation*(+1))\n",
    "  \n",
    "  return(u)\n",
    "}  \n",
    "\n",
    "\n",
    "#Transforms pca feature reduction to the given dimension.\n",
    "#If data_made_from=NULL the pca was contucted to the given data,\n",
    "#else the pca was contucted to the given data_made_from.\n",
    "pca_dimensionality_reduction<- function(data,dimension,pca_X_train,data_made_from=NULL){\n",
    "  \n",
    "  if(dimension>(length(data[1,]))){\n",
    "    print(\"ERROR: Dimension must be integer smaller than datas' dimension!!!\")\n",
    "    return(data)\n",
    "  }\n",
    "  \n",
    "  if(is.null(data_made_from)){\n",
    "    for(i in 1:length(data[1,])){\n",
    "      data[,i]=data[,i]-mean(data[,i])\n",
    "    }\n",
    "  }else{\n",
    "    data=data.frame(data)\n",
    "    data=data[,names(data_made_from)]\n",
    "    for(i in 1:length(data[1,])){\n",
    "      data[,i]=data[,i]-mean(data_made_from[,i])\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  #from the matrix whose columns contain the eigenvectors  keep the first X=dimension  \n",
    "  pca_X_train=array(pca_X_train[,1:dimension],dim =c(length(data[1,]),dimension))\n",
    "  \n",
    "  #multiply the data(in which first we subtract the mean valumn at every column) with  pca_X_train  \n",
    "  red_data=data.matrix(data)%*%data.matrix(pca_X_train)\n",
    "  return(red_data)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function fo doing the reduction-selection  \n",
    "feature_reduction <- function(merged_episodes,test_episodes_list,PCA_REDUCTION=FALSE,top_features=0\n",
    "                              ,thres=0.5,algo_choice=0,seed=0){\n",
    "  #if PCA_REDUCTION do pca feature extraction-reduction.  \n",
    "  if(PCA_REDUCTION){\n",
    "    \n",
    "    #save as Label-column the last column   \n",
    "    Risk_F=merged_episodes[,length(merged_episodes[1,])]\n",
    "    \n",
    "    #if is already a data_frame   \n",
    "    if(is.data.frame(merged_episodes)){\n",
    "      #keep the non zero columns  \n",
    "      merged_episodes=merged_episodes[,1:(length(merged_episodes[1,])-1)]                           \n",
    "      merged_episodes = merged_episodes[, colSums(merged_episodes != 0) > 0]\n",
    "    }else{\n",
    "      #keep the non zero columns, by using remove_zero_columns   \n",
    "      merged_episodes=remove_zero_columns(merged_episodes[,1:(length(merged_episodes[1,])-1)])\n",
    "    }\n",
    "    \n",
    "    #save the data that pca will contucted   \n",
    "    data_made_from=merged_episodes\n",
    "    \n",
    "    #eigenvectors    \n",
    "    pca_X_train=pca_function(merged_episodes)\n",
    "    \n",
    "    #make the dimensionality reduction for training_set  \n",
    "    merged_episodes=pca_dimensionality_reduction(merged_episodes,top_features,pca_X_train)\n",
    "    \n",
    "    #make the dimensionality reduction for testing_set    \n",
    "    for(i in 1:length(test_episodes_list)){\n",
    "      test_episodes_list[[i]]=pca_dimensionality_reduction(test_episodes_list[[i]],top_features\n",
    "                                                           ,pca_X_train,data_made_from)\n",
    "    }\n",
    "    \n",
    "    for(i in 1:length(test_episodes_list)){\n",
    "      test_episodes_list[[i]]=data.frame(test_episodes_list[[i]])\n",
    "    }\n",
    "    \n",
    "    merged_episodes=data.frame(merged_episodes,Risk_F)\n",
    "    \n",
    "  }else{\n",
    "    if(top_features>0){\n",
    "      #remove columns with all values equal to zero\n",
    "      merged_episodes = merged_episodes[, colSums(merged_episodes != 0) > 0]\n",
    "      \n",
    "      #keep the top features\n",
    "      merged_episodes = top_feature_selection(merged_episodes,top_features,seed=seed)\n",
    "      \n",
    "      #print(\"Merged_episodes after feature selection:\")\n",
    "      #print(head(merged_episodes))  \n",
    "      \n",
    "      #print(\"The names of the selected fearures are:\")\n",
    "      #print(names(merged_episodes))  \n",
    "    }else{\n",
    "      merged_episodes=best_feature_selection(merged_episodes,test_episodes_list,step,choice=algo_choice\n",
    "                                             ,acc_thres=thres)\n",
    "      \n",
    "    }\n",
    "    \n",
    "    #from test_episodes_list keep only the columns that instances_df has   \n",
    "    for(i in 1:length(test_episodes_list)){\n",
    "      test_episodes_list[[i]]=(test_episodes_list[[i]][,names(merged_episodes)])\n",
    "    } \n",
    "    \n",
    "  }\n",
    "  \n",
    "  #return the new merged_episodes and test_episodes_list    \n",
    "  MyList<- list(\"a\"=merged_episodes, \"b\"=test_episodes_list)\n",
    "  return(MyList)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for Random Forest \n",
    "\n",
    "The segmented data in combination\n",
    "with the risk quantification values are fed into a **Random Forests** algorithm as\n",
    "a training set to form a regression problem, which is based on the minimization\n",
    "of the mean squared error.\n",
    "\n",
    "**1) function: RFfit**\n",
    "\n",
    "**2) function: eval**\n",
    "\n",
    "**3) function: plot_fuct**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that uses Random Forest for predictive the target event(i.e. fault). \n",
    "RFfit <- function(train_episodes,seed,plotbool){\n",
    "  set.seed(seed) #for remaining the random output the same\n",
    "  \n",
    "  #Training with randomForest model using instances_df(made from training set)\n",
    "  #Random Forest R documentation and parameters explanation -> https://www.rdocumentation.org/packages/randomForest/versions/4.6-14/topics/randomForest  \n",
    "  #Random Forest idea -> https://www.youtube.com/watch?v=loNcrMjYh64  \n",
    "  my.rf <- randomForest(Risk_F ~ .,data=train_episodes,importance=TRUE, ntree=500) #(default ntree=500) \n",
    "  \n",
    "  #if ploot then print the tree informations      \n",
    "  if((plotbool==TRUE)){\n",
    "    result = tryCatch({\n",
    "      varImpPlot(my.rf) \n",
    "    }, error = function(e) {\n",
    "      print(\"Error at varImpPlot(my.rf)\")\n",
    "    }) \n",
    "    \n",
    "  }\n",
    "  \n",
    "  \n",
    "  \n",
    "  #if plootbool then print the tree informations  \n",
    "  if(plotbool==TRUE)\n",
    "  {\n",
    "    print(\"------------------------------------------------------------------------------------------------------\")   \n",
    "    for(i in 1:2){   \n",
    "      tryCatch(\n",
    "        expr = {\n",
    "          print(getTree(my.rf, i,labelVar=TRUE))\n",
    "        },\n",
    "        error = function(e){ \n",
    "          \n",
    "        }\n",
    "      )  \n",
    "    } \n",
    "    print(\"------------------------------------------------------------------------------------------------------\")   \n",
    "  }\n",
    "  \n",
    "  return(my.rf)      \n",
    "}     \n",
    "  \n",
    "#Function that evaluates the results of RandomForest, XGBoost, PLS, KNN.\n",
    "eval <- function(test_episodes_list,my_boost,acceptance_threshold=0.5\n",
    "                 ,USE_XGB=FALSE,USE_PLS=FALSE,USE_KNN=FALSE,IS_MAX =FALSE,path=\"\",N=1,moving_step=1,days=list()){\n",
    "  false_positives = 0\n",
    "  true_positives = 0\n",
    "  false_negatives = 0\n",
    "  i=0\n",
    "  counter=0\n",
    "  \n",
    "  sum=0\n",
    "  #for every episode in the episodes_list  \n",
    "  for(ep in test_episodes_list){\n",
    "    \n",
    "    \n",
    "    if(length(ep)>0){\n",
    "      counter=counter+1\n",
    "      \n",
    "      ep = ep[ , !(names(ep) %in% c(\"Timestamps\"))]\n",
    "      ep = ep[ , !(names(ep) %in% c(\"Risk_F\"))]\n",
    "      #print(\"For episode:\")\n",
    "      #print(ep)\n",
    "      #print(ep)\n",
    "      \n",
    "      #keep the positions of the TRUEs  \n",
    "      #TRUE if prediction is bigger than the acceptance threshold,else FALSE\n",
    "      if(USE_XGB){\n",
    "        Prediction <- predict(my_boost, as.matrix(ep)) #make the prediction of the episode\n",
    "        pred_indeces = as.numeric(which((Prediction >= acceptance_threshold)==TRUE)) #acceptance_threshold=0.5 which(train_labels %in% c(1))\n",
    "      }else if(USE_PLS){\n",
    "        Prediction=(data.matrix(as.matrix(ep))) %*% data.matrix(my_boost$reg.coefs[-1])\n",
    "        Prediction[,1]=Prediction[,1]+as.numeric(my_boost$reg.coefs[1])\n",
    "        pred_indeces = as.numeric(which((Prediction >= acceptance_threshold)==TRUE)) #acceptance_threshold=0.5\n",
    "      }else if(USE_KNN){\n",
    "        if(i==0){\n",
    "          before_ep=1\n",
    "          Prediction=my_boost[before_ep:(before_ep+length(ep[,1])-1)]\n",
    "          i=10\n",
    "        }\n",
    "        else{\n",
    "          Prediction=my_boost[before_ep:(before_ep+length(ep[,1])-1)]\n",
    "        }\n",
    "        before_ep=before_ep+length(ep[,1])\n",
    "        pred_indeces = as.numeric(which((Prediction >= acceptance_threshold)==TRUE))\n",
    "        \n",
    "      }else{\n",
    "        Prediction <- predict(my_boost, ep) #make the prediction of the episode\n",
    "        pred_indeces = as.numeric(names(Prediction[Prediction >= acceptance_threshold])) #acceptance_threshold=0.5\n",
    "      }\n",
    "      \n",
    "      \n",
    "      #print(\"The pred_indeces before:\")\n",
    "      #print(\"sum\")\n",
    "      #print(sum)\n",
    "      temp=ceiling(sum/moving_step) \n",
    "      #print(\"start\")\n",
    "      start=temp*moving_step-sum\n",
    "      #print(start)\n",
    "      #ep_legth = length(Prediction)\n",
    "      ep_legth=days[[counter]]\n",
    "      #print(\"ep\")\n",
    "      #print(ep_legth)\n",
    "      #print(\"length\")\n",
    "      #print(ep_legth)\n",
    "      if(start>=0&&counter>1){\n",
    "        pred_indeces=((pred_indeces-1)*moving_step+start+1)\n",
    "      }else{\n",
    "        pred_indeces=((pred_indeces-1)*moving_step+N)\n",
    "      }\n",
    "      # print(\"pred_indeces\")\n",
    "      #print(pred_indeces)\n",
    "      #print(\"The pred_indeces:\")\n",
    "      #print(pred_indeces)\n",
    "      \n",
    "      #if there is warnings before the max interval from the failure(target event)  \n",
    "      if(length(pred_indeces[pred_indeces < (ep_legth-(max_warning_interval))]) > 0){\n",
    "        #increase false positives by the number of these warnings    \n",
    "        false_positives = false_positives + length(pred_indeces[pred_indeces < (ep_legth-(max_warning_interval))])\n",
    "      } \n",
    "      \n",
    "      #if there is warnings after the max and before the min interval from the failure(target event)     \n",
    "      if(length(pred_indeces[pred_indeces >= (ep_legth-(max_warning_interval)) & pred_indeces <= (ep_legth-min_warning_interval)]) > 0){\n",
    "        true_positives = true_positives + 1 #increase true positives by 1\n",
    "        #if there is no correct warning    \n",
    "      } else {\n",
    "        false_negatives = false_negatives + 1 #increase false negatives by 1\n",
    "      }\n",
    "      \n",
    "      sum=sum+days[[counter]]\n",
    "      #print(\"------------------------------------------------------------------------------------------------------\")    \n",
    "    }\n",
    "  }\n",
    "  \n",
    "  \n",
    "  precision = true_positives/(true_positives+false_positives) #calculate the precision of the model\n",
    "  if((true_positives+false_positives) == 0){\n",
    "    precision = 0\n",
    "  }\n",
    "  recall = true_positives/length(test_episodes_list) #calculate recall of the model\n",
    "  \n",
    "  F1 = 2*((precision*recall)/(precision+recall)) #calculate F1 score of the model\n",
    "  if((precision+recall) == 0){\n",
    "    F1 = 0\n",
    "  }\n",
    "  \n",
    "  #prints \n",
    "  #if(!csv){\n",
    "  if(TRUE){  \n",
    "    cat(paste(\"dataset:\",argv$test,\"\\ntrue_positives:\", true_positives,\"\\nfalse_positives:\", false_positives,\"\\nfalse_negatives:\", false_negatives,\"\\nprecision:\", precision,\"\\nrecall:\", recall,\"\\nF1:\", F1, \"\\n\"))\n",
    "    if(IS_MAX){\n",
    "      write(paste(\"milw:\",milw,\" midpoint:\", midpoint,\" stepness:\", s,\n",
    "                  \" MIN:\", min_warning_interval,\" MAX:\", max_warning_interval,\"\\n\")\n",
    "            ,file=path,append=TRUE)\n",
    "      write(paste(\"dataset:\",path,\"\\ntrue_positives:\", true_positives,\"\\nfalse_positives:\"\n",
    "                  , false_positives,\"\\nfalse_negatives:\", false_negatives,\"\\nprecision:\"\n",
    "                  , precision,\"\\nrecall:\", recall,\"\\nF1:\", F1,\"\\n\"),file=path,append=TRUE)\n",
    "    }  \n",
    "  } else {\n",
    "    cat(paste(argv$test,\",\",true_positives,\",\",false_positives,\",\",false_negatives,\",\",precision,\",\",recall,\",\",F1,\",\",argv$fet,\",\",argv$tet,\",\",argv$rre,\",\",argv$rfe,\",\",argv$kofe,\",\",argv$mili,\",\",argv$milt,\",\",argv$fs,\",\",argv$top,\",\",argv$rer,\",\",argv$fer,\",\",argv$seed,\",\",argv$steepness,\",\",argv$pthres,\",\",argv$milw,\",\",argv$milthres,\",\",argv$midpoint,\",\",argv$minwint,\",\",argv$maxwint,\"\\n\",sep=\"\"))\n",
    "  }\n",
    "  \n",
    "  print(\"F1 score is:\")\n",
    "  print(F1)\n",
    "}\n",
    "\n",
    "#plot function\n",
    "plot_fuct <- function(test_episodes_list, episode_index, my.rf){\n",
    "  test_episodes = test_episodes_list[[episode_index]][ , !(names(test_episodes_list[[episode_index]]) %in% c(\"Timestamps\"))]\n",
    "  Prediction <- predict(my.rf, test_episodes)\n",
    "  results = data.frame(Risk_F=test_episodes$Risk_F,num_Prediction=as.numeric(Prediction))\n",
    "  mse = mean((Prediction-test_episodes$Risk_F)^2)\n",
    "  \n",
    "  chart =ggplot(results,aes((1:nrow(results)))) +\n",
    "    # geom_rect(aes(xmin = ceiling(nrow(df_test)/2), xmax = nrow(df_test), ymin = -Inf, ymax = Inf),\n",
    "    #           fill = \"yellow\", alpha = 0.003) +\n",
    "    geom_line(aes(y = Risk_F, colour = \"Actual\")) +\n",
    "    geom_line(aes(y = num_Prediction, colour=\"Predicted\")) +\n",
    "    labs(colour=\"Lines\") +\n",
    "    xlab(\"Segments\") +\n",
    "    ylab('Risk (F)') +\n",
    "    ggtitle(\"Risk Prediction\") + # (RR_KF_2YEARS_PAT08)\n",
    "    theme(plot.title = element_text(hjust = 0.5)) +\n",
    "    geom_text(aes(label = paste(\"MSE=\",round(mse,3)), x = 20, y = 1), hjust = -2, vjust = 6, color=\"black\", size=4) #add MSE label\n",
    "  \n",
    "  # Disable clip-area so that the MSE is shown in the plot\n",
    "  gt <- ggplot_gtable(ggplot_build(chart))\n",
    "  gt$layout$clip[gt$layout$name == \"panel\"] <- \"off\"\n",
    "  grid.draw(gt)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use PCA top_features must be less than: 2415 \n",
      "-------------------------------------------------------------\n",
      "dataset: C:/Users/petsi/Documents/ptyxiakh/testing_my_dataset_150.csv \n",
      "true_positives: 14 \n",
      "false_positives: 10 \n",
      "false_negatives: 8 \n",
      "precision: 0.583333333333333 \n",
      "recall: 0.636363636363636 \n",
      "F1: 0.608695652173913 \n",
      "[1] \"F1 score is:\"\n",
      "[1] 0.6086957\n"
     ]
    }
   ],
   "source": [
    "#Run Random Forest algorithm and print the results.\n",
    "\n",
    "#feature selection for Random Forest\n",
    "merged_episodes_rf=merged_episodes1                    \n",
    "test_episodes_list_rf=test_episodes_list\n",
    "threas_RF=0.4\n",
    "\n",
    "top_features=500          \n",
    "FEATURE_SELECTION=TRUE\n",
    "PCA_REDUCTION=FALSE\n",
    "\n",
    "cat(\"If you use PCA top_features must be less than:\",length(merged_episodes_rf[1,])-2,\"\\n\")\n",
    "\n",
    "if(FEATURE_SELECTION){\n",
    "  red_list=feature_reduction(merged_episodes_rf,test_episodes_list_rf,PCA_REDUCTION,top_features,thres=threas_RF,algo_choice=1,seed)\n",
    "  \n",
    "  merged_episodes_rf=red_list[1]$a\n",
    "  test_episodes_list_rf=red_list[2]$b\n",
    "}  \n",
    "\n",
    "#Run Random Forest algorithm and print the results.  \n",
    "my.rf = RFfit(merged_episodes_rf,seed,FALSE)\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "rf_results=eval(test_episodes_list_rf,my.rf,acceptance_threshold=threas_RF,N=N,moving_step = moving_step,days=days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for XGBoost\n",
    "\n",
    "**function: XGBoostfit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that uses XGBoost for predictive the target event(i.e. fault). \n",
    "XGBoostfit <- function(train_episodes,seed,plotbool){\n",
    "  set.seed(seed)\n",
    "  \n",
    "  dtrain <- xgb.DMatrix(data=as.matrix(train_episodes[,1:(length(train_episodes[1,])-1)]),label=as.matrix(train_episodes[,(length(train_episodes[1,]))]))\n",
    "  my.xgb <- xgb.train(data = dtrain, nthread = 2, eta=0.6, nrounds = 10, objective = \"binary:logistic\",verbose = 2)\n",
    "  \n",
    "  #if plootbool then print the tree informations  \n",
    "  if(plotbool==TRUE)\n",
    "  {\n",
    "    print(\"------------------------------------------------------------------------------------------------------\")\n",
    "    tryCatch(\n",
    "      expr = {\n",
    "        print(xgb.plot.tree(model = my.xgb))\n",
    "      },\n",
    "      error = function(e){ \n",
    "        print(xgb.dump(my.xgb, with_stats = T))\n",
    "      }\n",
    "    )  \n",
    "    \n",
    "  } \n",
    "  return(my.xgb)\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use PCA top_features must be less than: 2415 \n",
      "-------------------------------------------------------------\n",
      "dataset: C:/Users/petsi/Documents/ptyxiakh/testing_my_dataset_150.csv \n",
      "true_positives: 18 \n",
      "false_positives: 13 \n",
      "false_negatives: 4 \n",
      "precision: 0.580645161290323 \n",
      "recall: 0.818181818181818 \n",
      "F1: 0.679245283018868 \n",
      "[1] \"F1 score is:\"\n",
      "[1] 0.6792453\n"
     ]
    }
   ],
   "source": [
    "#Run XGB algorithm and print the results.\n",
    "\n",
    "#feature selection for XGBoost\n",
    "merged_episodes_XGB=merged_episodes1\n",
    "test_episodes_list_XGB=test_episodes_list\n",
    "threas_XGB=0.4\n",
    "\n",
    "top_features=70      \n",
    "FEATURE_SELECTION=TRUE\n",
    "PCA_REDUCTION=FALSE\n",
    "\n",
    "cat(\"If you use PCA top_features must be less than:\",length(merged_episodes_XGB[1,])-2,\"\\n\")\n",
    "\n",
    "if(FEATURE_SELECTION){\n",
    "  red_list=feature_reduction(merged_episodes_XGB,test_episodes_list_XGB,PCA_REDUCTION,top_features,thres=threas_XGB,algo_choice=2,seed=seed)\n",
    "  \n",
    "  merged_episodes_XGB=red_list[1]$a\n",
    "  test_episodes_list_XGB=red_list[2]$b\n",
    "}\n",
    "\n",
    "#Run XGBoost algorithm and print the results.\n",
    "my.xgb = XGBoostfit(merged_episodes_XGB,seed,FALSE)\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "xgb_results=eval(test_episodes_list_XGB,my.xgb,threas_XGB,USE_XGB=TRUE,N=N,moving_step = moving_step,days=days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use PCA top_features must be less than: 2415 \n",
      "-------------------------------------------------------------\n",
      "dataset: C:/Users/petsi/Documents/ptyxiakh/testing_my_dataset_150.csv \n",
      "true_positives: 13 \n",
      "false_positives: 13 \n",
      "false_negatives: 9 \n",
      "precision: 0.5 \n",
      "recall: 0.590909090909091 \n",
      "F1: 0.541666666666667 \n",
      "[1] \"F1 score is:\"\n",
      "[1] 0.5416667\n"
     ]
    }
   ],
   "source": [
    "#Run PLS regression algorithm and print the results.\n",
    "\n",
    "#feature selection for PLS\n",
    "merged_episodes_PLS=merged_episodes1\n",
    "test_episodes_list_PLS=test_episodes_list\n",
    "threas_PLS=0.4\n",
    "\n",
    "top_features=50            \n",
    "FEATURE_SELECTION=TRUE  \n",
    "PCA_REDUCTION=FALSE\n",
    "\n",
    "cat(\"If you use PCA top_features must be less than:\",length(merged_episodes_PLS[1,])-2,\"\\n\")\n",
    "\n",
    "if(FEATURE_SELECTION){\n",
    "  red_list=feature_reduction(merged_episodes_PLS,test_episodes_list_PLS,PCA_REDUCTION,top_features,seed)\n",
    "  \n",
    "  merged_episodes_PLS=red_list[1]$a\n",
    "  test_episodes_list_PLS=red_list[2]$b\n",
    "}else{\n",
    "  for(i in 1:length(test_episodes_list_LPS)){\n",
    "    test_episodes_list_PLS[[i]]=(test_episodes_list_PLS[[i]][,names(merged_episodes_PLS)])\n",
    "  } \n",
    "}\n",
    "\n",
    "num_of_features=length(merged_episodes_PLS[1,])\n",
    "my.pls = plsreg1(merged_episodes_PLS[,-num_of_features],merged_episodes_PLS[,num_of_features], comps=12, crosval=TRUE)\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "pls_results=eval(test_episodes_list_PLS,my.pls,acceptance_threshold=threas_PLS,USE_PLS=TRUE,N=N,moving_step = moving_step,days=days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use PCA top_features must be less than: 2415 \n",
      "-------------------------------------------------------------\n",
      "dataset: C:/Users/petsi/Documents/ptyxiakh/testing_my_dataset_150.csv \n",
      "true_positives: 17 \n",
      "false_positives: 22 \n",
      "false_negatives: 5 \n",
      "precision: 0.435897435897436 \n",
      "recall: 0.772727272727273 \n",
      "F1: 0.557377049180328 \n",
      "[1] \"F1 score is:\"\n",
      "[1] 0.557377\n"
     ]
    }
   ],
   "source": [
    "#Run KNN regression algorithm and print the results.\n",
    "\n",
    "\n",
    "#feature selection for KNN regression\n",
    "merged_episodes_KNN=merged_episodes1\n",
    "test_episodes_list_KNN=test_episodes_list\n",
    "threas_KNN=0.3\n",
    "\n",
    "top_features=10            \n",
    "FEATURE_SELECTION=TRUE\n",
    "PCA_REDUCTION=TRUE\n",
    "\n",
    "cat(\"If you use PCA top_features must be less than:\",length(merged_episodes_KNN[1,])-2,\"\\n\")\n",
    "\n",
    "if(FEATURE_SELECTION){\n",
    "  red_list=feature_reduction(merged_episodes_KNN,test_episodes_list_KNN,PCA_REDUCTION,top_features)\n",
    "  \n",
    "  merged_episodes_KNN=red_list[1]$a\n",
    "  test_episodes_list_KNN=red_list[2]$b\n",
    "}\n",
    "\n",
    "\n",
    "merged_testing = ldply(test_episodes_list_KNN, data.frame) #merging all lists of episodes list to one data frame\n",
    "merged_testing = merged_testing[ , !(names(merged_testing) %in% c(\"Timestamps\"))] #delete column Timestamps\n",
    "merged_testing = merged_testing[ , !(names(merged_testing) %in% c(\"Risk_F\"))]\n",
    "\n",
    "K=3\n",
    "knn.fast<- knn(x=as.matrix(merged_episodes_KNN[ , !(names(merged_episodes_KNN) %in% c(\"Risk_F\"))]) ,xnew=as.matrix(merged_testing) , y=(merged_episodes_KNN$Risk_F), k=K,dist.type = \"euclidean\", type = \"R\", method = \"average\")\n",
    "prediction_knn_fast=(as.numeric(knn.fast))\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "KNNfast_results=eval(test_episodes_list_KNN,prediction_knn_fast,threas_KNN,USE_KNN = TRUE,N=N,moving_step = moving_step,days=days) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "   ### 1)LSTM\n",
    "\n",
    "   ### 2)CNN\n",
    "\n",
    "   ### 3)MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for the implemantation of the selected NN\n",
    "**1) function: make_model**\n",
    "\n",
    "**2) function: fit_model**\n",
    "\n",
    "**2) function: eval_model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for creating the NN.\n",
    "make_model<-function(inputs_shape,USE_LSTM=FALSE,USE_CNN=FALSE,lr=0.01){\n",
    "  \n",
    "  hidden_layer1=128\n",
    "  dense_layer1=128\n",
    "  \n",
    "  if(USE_LSTM){\n",
    "    modeling = keras_model_sequential() %>%   \n",
    "      layer_lstm(units=hidden_layer1, input_shape=inputs_shape ,return_sequences=FALSE)%>%\n",
    "      #layer_lstm(units=hidden_layer1/4, input_shape=inputs_shape,return_sequences = TRUE) %>%\n",
    "      #layer_dropout(0.25)%>%\n",
    "      #layer_lstm(units=hidden_layer1/2,return_sequences = FALSE) %>%\n",
    "      \n",
    "      \n",
    "      layer_dense(units=1)\n",
    "    \n",
    "  }else if(USE_CNN){\n",
    "    number_of_timestamps=inputs_shape[1] \n",
    "    print(number_of_timestamps)  \n",
    "    print(inputs_shape)\n",
    "    kern_size=3\n",
    "    if(number_of_timestamps<3){\n",
    "      kern_size=number_of_timestamps\n",
    "    }\n",
    "    modeling = keras_model_sequential() %>%   \n",
    "      layer_conv_1d(filters=number_of_timestamps,kernel_size=kern_size,input_shape=inputs_shape,activation =\"relu\") %>%\n",
    "      layer_conv_1d(filters=number_of_timestamps,kernel_size=kern_size,activation =\"relu\") %>%\n",
    "      #layer_max_pooling_1d(pool_size=2) %>%\n",
    "      #layer_conv_1d(filters=2*X*M,kernel_size=3,activation =\"relu\") %>%\n",
    "      #layer_conv_1d(filters=2*X*M,kernel_size=3,activation =\"relu\") %>%\n",
    "      layer_global_average_pooling_1d()%>%\n",
    "      layer_dropout(0.2)%>%\n",
    "      layer_dense(units=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "  }else{\n",
    "    modeling = keras_model_sequential() %>%   \n",
    "      \n",
    "      layer_dense(units=dense_layer1/2, input_shape=inputs_shape, activation = \"relu\") %>%\n",
    "      #layer_dropout(0.25)%>%\n",
    "      layer_dense(units=dense_layer1, activation = \"relu\") %>%\n",
    "      layer_dropout(0.25)%>%\n",
    "      layer_dense(units=dense_layer1/2, activation = \"relu\") %>%\n",
    "      layer_dropout(0.25)%>%\n",
    "      layer_dense(units=1)\n",
    "    \n",
    "  }\n",
    "  \n",
    "  \n",
    "  learningrate=lr\n",
    "  opt = optimizer_adam(lr=learningrate)#, momentum=0.9, nesterov=TRUE)\n",
    "  modeling %>% compile(loss = 'mse',\n",
    "                       optimizer = opt,\n",
    "                       metrics = list(\"mean_absolute_error\") #\"mean_absolute_error\" \n",
    "  )\n",
    "  \n",
    "  modeling %>% summary()\n",
    "  return(modeling)\n",
    "  \n",
    "}\n",
    "\n",
    "#Function for fitting the NN with the training_set.\n",
    "fit_model<-function(modeling,train_set,train_labels,epoch_size,batchs_size,classes_weights,shuffling = FALSE){ \n",
    "  modeling %>% fit(train_set,train_labels, epochs=epoch_size,batch_size=batchs_size,shuffle = shuffling)#,class_weight=classes_weights\n",
    "  return(modeling)\n",
    "}\n",
    "\n",
    "#Function for evaluating the NN with the testing_set(or the predicrions that already have been calculated).\n",
    "eval_model <- function(test_episodes_list=list(),seed=0,modeling=NULL,acceptance_threshold=0.5\n",
    "                       ,USE_LSTM=FALSE,USE_CNN=FALSE,number_of_timestamps=1,predictions=list(),IS_MAX=FALSE\n",
    "                       ,moving_step=1,days=list(),N=N){\n",
    " \n",
    "    set.seed(seed) #for remaining the random output the same\n",
    "    \n",
    "    counter=0\n",
    "    sum=0\n",
    "    false_positives = 0\n",
    "    true_positives = 0\n",
    "    false_negatives = 0                         \n",
    "    i=0\n",
    "    #acceptance_threshold=0.7\n",
    "    if(length(test_episodes_list)==0){\n",
    "      test_episodes_list=predictions\n",
    "    }\n",
    "    #for every episode in the episodes_list  \n",
    "    for(ep in test_episodes_list){\n",
    "      i=i+1\n",
    "      counter=counter+1\n",
    "      #print(i)\n",
    "      if(length(predictions)==0){\n",
    "        ep = ep[ , !(names(ep) %in% c(\"Timestamps\"))]\n",
    "        ep = ep[ , !(names(ep) %in% c(\"Risk_F\"))]\n",
    "        \n",
    "        test_set = data.matrix(ep[,1:length(ep[1,])])\n",
    "        \n",
    "        if(USE_LSTM||USE_CNN){\n",
    "          test_set = array(test_set, dim=c(length(ep[,1]),number_of_timestamps,length(ep[1,])/number_of_timestamps))\n",
    "        }else{\n",
    "          test_set = array(test_set, dim=c(length(ep[,1]),length(ep[1,])))\n",
    "        }\n",
    "        \n",
    "        \n",
    "        Prediction = modeling %>% predict(test_set) #make the prediction of the episode\n",
    "        ep_legth = length(Prediction)\n",
    "        pred_indeces=which(Prediction %in% c(Prediction[Prediction >= acceptance_threshold])) \n",
    "      }\n",
    "      else{\n",
    "        Prediction=predictions[[i]]\n",
    "        ep_legth = length(Prediction)\n",
    "        pred_indeces=which(Prediction %in% c(Prediction[Prediction == TRUE])) \n",
    "        #print(pred_indeces)\n",
    "      }\n",
    "      \n",
    "      temp=ceiling(sum/moving_step) \n",
    "      #print(\"start\")\n",
    "      start=temp*moving_step-sum\n",
    "      #print(start)\n",
    "      #ep_legth = length(Prediction)\n",
    "      ep_legth=days[[counter]]\n",
    "      #print(\"ep\")\n",
    "      #print(ep_legth)\n",
    "      #print(\"length\")\n",
    "      #print(ep_legth)\n",
    "      if(start>=0&&counter>1){\n",
    "        pred_indeces=((pred_indeces-1)*moving_step+start+1)\n",
    "      }else{\n",
    "        pred_indeces=((pred_indeces-1)*moving_step+N)\n",
    "      }\n",
    "      \n",
    "      \n",
    "      \n",
    "      #if there is warnings before the max interval from the failure(target event)  \n",
    "      if(length(pred_indeces[pred_indeces < (ep_legth-(max_warning_interval))]) > 0){\n",
    "        #increase false positives by the number of these warnings    \n",
    "        false_positives = false_positives + length(pred_indeces[pred_indeces < (ep_legth-(max_warning_interval))])\n",
    "      } \n",
    "      \n",
    "      #if there is warnings after the max and before the min interval from the failure(target event)     \n",
    "      if(length(pred_indeces[pred_indeces >= (ep_legth-(max_warning_interval)) & pred_indeces <= (ep_legth-min_warning_interval)]) > 0){\n",
    "        true_positives = true_positives + 1 #increase true positives by 1\n",
    "        #if there is no correct warning    \n",
    "      } else {\n",
    "        false_negatives = false_negatives + 1 #increase false negatives by 1\n",
    "      }\n",
    "      sum=sum+days[[counter]]\n",
    "    }\n",
    "    \n",
    "    \n",
    "    precision = true_positives/(true_positives+false_positives) #calculate the precision of the model\n",
    "    if((true_positives+false_positives) == 0){\n",
    "      precision = 0\n",
    "    }\n",
    "    recall = true_positives/length(test_episodes_list) #calculate recall of the model\n",
    "    \n",
    "    F1 = 2*((precision*recall)/(precision+recall)) #calculate F1 score of the model\n",
    "    if((precision+recall) == 0){\n",
    "      F1 = 0\n",
    "    }\n",
    "    \n",
    "    #prints \n",
    "    if(TRUE){\n",
    "      cat(paste(\"dataset:\",argv$test,\"\\ntrue_positives:\", true_positives,\"\\nfalse_positives:\", false_positives,\"\\nfalse_negatives:\", false_negatives,\"\\nprecision:\", precision,\"\\nrecall:\", recall,\"\\nF1:\", F1, \"\\n\"))\n",
    "      if(IS_MAX){\n",
    "        write(paste(\"milw:\",milw,\" midpoint:\", midpoint,\" stepness:\", s,\n",
    "                    \" MIN:\", min_warning_interval,\" MAX:\", max_warning_interval,\"\\n\")\n",
    "              ,file=path,append=TRUE)\n",
    "        write(paste(\"dataset:\",path,\"\\ntrue_positives:\", true_positives,\"\\nfalse_positives:\"\n",
    "                    , false_positives,\"\\nfalse_negatives:\", false_negatives,\"\\nprecision:\"\n",
    "                    , precision,\"\\nrecall:\", recall,\"\\nF1:\", F1,\"\\n\"),file=path,append=TRUE)\n",
    "      } \n",
    "    } else {\n",
    "      cat(paste(argv$test,\",\",true_positives,\",\",false_positives,\",\",false_negatives,\",\",precision,\",\",recall,\",\",F1,\",\",argv$fet,\",\",argv$tet,\",\",argv$rre,\",\",argv$rfe,\",\",argv$kofe,\",\",argv$mili,\",\",argv$milt,\",\",argv$fs,\",\",argv$top,\",\",argv$rer,\",\",argv$fer,\",\",argv$seed,\",\",argv$steepness,\",\",argv$pthres,\",\",argv$milw,\",\",argv$milthres,\",\",argv$midpoint,\",\",argv$minwint,\",\",argv$maxwint,\"\\n\",sep=\"\"))\n",
    "    }\n",
    "    \n",
    "    print(\"F1 score is:\")\n",
    "    print(F1)\n",
    "    return(F1)\n",
    "\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the necessary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "#default: MLP \n",
    "USE_LSTM=FALSE\n",
    "USE_CNN=FALSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you use PCA top_features must be less than: 2415 \n"
     ]
    }
   ],
   "source": [
    "#feature selection for NN\n",
    "merged_episodes_model=merged_episodes1\n",
    "test_episodes_list_model=test_episodes_list\n",
    "threas_MODEL=0.4\n",
    "\n",
    "top_features=500            \n",
    "FEATURE_SELECTION=TRUE\n",
    "PCA_REDUCTION=FALSE\n",
    "\n",
    "cat(\"If you use PCA top_features must be less than:\",length(merged_episodes_model[1,])-2,\"\\n\")\n",
    "\n",
    "if(FEATURE_SELECTION){\n",
    "  red_list=feature_reduction(merged_episodes_model,test_episodes_list_model,PCA_REDUCTION,top_features)\n",
    "  \n",
    "  merged_episodes_model=red_list[1]$a\n",
    "  test_episodes_list_model=red_list[2]$b\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "Ytrain=array(merged_episodes_model$Risk_F,dim=c(length(merged_episodes_model[,1]),1,1))\n",
    "X=merged_episodes_model[ , !(names(merged_episodes_model) %in% c(\"Risk_F\"))] #delete column Risk_F\n",
    "Xtrain=data.matrix(X)\n",
    "\n",
    "xdimtr=length(Xtrain[,1])  \n",
    "ydimtr=length(Xtrain[1,])\n",
    "\n",
    "if(USE_LSTM||USE_CNN){\n",
    "  number_of_timestamps=5  \n",
    "  Xtrain = array(Xtrain, dim=c(xdimtr,number_of_timestamps,ydimtr/number_of_timestamps))\n",
    "  inputs_shape=c(number_of_timestamps,ydimtr/number_of_timestamps)\n",
    "}else{\n",
    "  Xtrain = array(Xtrain, dim=c(xdimtr,ydimtr))\n",
    "  inputs_shape=c(ydimtr)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make-Fit-Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "________________________________________________________________________________\n",
      "Layer (type)                        Output Shape                    Param #     \n",
      "================================================================================\n",
      "dense_44 (Dense)                    (None, 64)                      32064       \n",
      "________________________________________________________________________________\n",
      "dense_45 (Dense)                    (None, 128)                     8320        \n",
      "________________________________________________________________________________\n",
      "dropout_24 (Dropout)                (None, 128)                     0           \n",
      "________________________________________________________________________________\n",
      "dense_46 (Dense)                    (None, 64)                      8256        \n",
      "________________________________________________________________________________\n",
      "dropout_25 (Dropout)                (None, 64)                      0           \n",
      "________________________________________________________________________________\n",
      "dense_47 (Dense)                    (None, 1)                       65          \n",
      "================================================================================\n",
      "Total params: 48,705\n",
      "Trainable params: 48,705\n",
      "Non-trainable params: 0\n",
      "________________________________________________________________________________\n",
      "dataset: C:/Users/petsi/Documents/ptyxiakh/testing_my_dataset_150.csv \n",
      "true_positives: 15 \n",
      "false_positives: 12 \n",
      "false_negatives: 7 \n",
      "precision: 0.555555555555556 \n",
      "recall: 0.681818181818182 \n",
      "F1: 0.612244897959184 \n",
      "[1] \"F1 score is:\"\n",
      "[1] 0.6122449\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.612244897959184"
      ],
      "text/latex": [
       "0.612244897959184"
      ],
      "text/markdown": [
       "0.612244897959184"
      ],
      "text/plain": [
       "[1] 0.6122449"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "modeling=make_model(inputs_shape,USE_LSTM,USE_CNN,lr=0.0005)\n",
    "modeling=fit_model(modeling,Xtrain,Ytrain,25,4)\n",
    "if(USE_LSTM||USE_CNN){\n",
    "    eval_model(test_episodes_list_model,seed,modeling,0.4,USE_LSTM,USE_CNN,number_of_timestamps,N=N,moving_step = moving_step,days=days)\n",
    "}else{\n",
    "    eval_model(test_episodes_list_model,seed,modeling,0.4,N=N,moving_step = moving_step,days=days)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Over-Under sampling\n",
    "\n",
    "**function: sampling_NN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling<-function(train_set,test_set,train_labels=NULL,times=0,repeats=0\n",
    "                      ,USE_OVERSAMPLING=FALSE,USE_UNDERSAMPLING=FALSE\n",
    "                      ,sample_thred=0.5,choice=\"\"){\n",
    "  \n",
    "  pp1=which(train_labels >= sample_thred) #positions of data that have Label>=sample_thred(belong to the TRUE class)\n",
    "  pp0=which(train_labels < sample_thred) #positions of data that have Label>=sample_thred(belong to the FALSE class)\n",
    "  #print(length(pp1))\n",
    "  #print(length(pp0))\n",
    "  \n",
    "  #If pp1>pp0 change them cause we want pp0 always be the class-Label with the most data  \n",
    "  if(length(pp1)>length(pp0)){\n",
    "    temp=pp1\n",
    "    pp1=pp0\n",
    "    pp0=temp\n",
    "  }\n",
    "  \n",
    "  #an list to save the results\n",
    "  sum_of_preds=list()\n",
    "  xdimtest=length(test_set)  \n",
    "  for(i in 1:xdimtest){\n",
    "    sum_of_preds[i]=list()\n",
    "  }\n",
    "  flag=FALSE #FALSE if for an episode of test_set no prediction made yet, else TRUE\n",
    "  \n",
    "  #for repeats times    \n",
    "  for(i in 1:repeats){\n",
    "    if(i==2){\n",
    "      flag=TRUE\n",
    "    }\n",
    "    #Use over_sampling or under_sampling so the number of data of each class to be the same  \n",
    "    if(USE_OVERSAMPLING){\n",
    "      pp01=pp0\n",
    "      pp1=sample(pp1, length(pp0), replace=TRUE)\n",
    "      pall=sample(c(pp01,pp1), times*length(c(pp01,pp1)), replace=TRUE)\n",
    "    }\n",
    "    else if(USE_UNDERSAMPLING){\n",
    "      pp01=sample(pp0, length(pp1), replace=FALSE)\n",
    "      pall=sample(c(pp01,pp1), times*length(c(pp01,pp1)), replace=TRUE)\n",
    "    }\n",
    "    else{\n",
    "      return(NULL)\n",
    "    }\n",
    "    \n",
    "    #print((pall))  \n",
    "    \n",
    "    train_set_i=train_set[pall,]\n",
    "    \n",
    "    return(train_set_i)\n",
    "    \n",
    "    train_labels_i=train_labels[pall]\n",
    "    \n",
    "    #make the model you chose with the new sampled_data  \n",
    "    modeling_sample=make_model(inputs_shape,USE_LSTM,USE_CNN,lr=0.001)\n",
    "    \n",
    "    #fit the model\n",
    "    modeling_sample=fit_model(modeling_sample,train_set_i,train_labels_i,20,8)#,shuffling = TRUE)#,list(\"0\"=0.1,\"1\"=2))\n",
    "    \n",
    "    count=0\n",
    "    \n",
    "    #for every episode in test_set \n",
    "    for(ep in test_set){\n",
    "      ep = ep[ , !(names(ep) %in% c(\"Timestamps\"))]\n",
    "      ep = ep[ , !(names(ep) %in% c(\"Risk_F\"))]\n",
    "      \n",
    "      if(USE_LSTM||USE_CNN){ \n",
    "        number_of_timestamps=inputs_shape[1]\n",
    "        test_ep= data.matrix(ep[,1:length(ep[1,])])\n",
    "        test_ep = array(test_ep, dim=c(length(ep[,1]),number_of_timestamps,length(ep[1,])/number_of_timestamps))\n",
    "      }\n",
    "      else{\n",
    "        test_ep= data.matrix(ep[,1:length(ep[1,])])\n",
    "      }  \n",
    "      \n",
    "      \n",
    "      count=count+1\n",
    "      \n",
    "      #make the predictions of the current model   \n",
    "      Prediction = modeling_sample %>% predict(test_ep)\n",
    "      \n",
    "      #add the result of the predictions to the sum_of_preds    \n",
    "      if(flag){\n",
    "        sum_of_preds[count]=list(sum_of_preds[[count]]+Prediction)\n",
    "      }\n",
    "      else{\n",
    "        sum_of_preds[count]=list(Prediction)\n",
    "      }\n",
    "    }\n",
    "    \n",
    "  }\n",
    "  \n",
    "  #return  the mean of the sum_of_preds\n",
    "  for(i in 1:xdimtest){\n",
    "    sum_of_preds[i]=list(sum_of_preds[[i]]/repeats)\n",
    "  }\n",
    "  \n",
    "  return(sum_of_preds)\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the selected Classifier with over-under sampled training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "dataset: C:/Users/petsi/Documents/ptyxiakh/testing_my_dataset_150.csv \n",
      "true_positives: 16 \n",
      "false_positives: 25 \n",
      "false_negatives: 6 \n",
      "precision: 0.390243902439024 \n",
      "recall: 0.727272727272727 \n",
      "F1: 0.507936507936508 \n",
      "[1] \"F1 score is:\"\n",
      "[1] 0.5079365\n"
     ]
    }
   ],
   "source": [
    "train_labels=array(merged_episodes1$Risk_F,dim=c(length(merged_episodes1[,1]),1,1))\n",
    "\n",
    "\n",
    "train_sample=sampling(merged_episodes1,test_episodes_list_XGB,train_labels=train_labels,times=1,repeats=0\n",
    "                                          ,USE_OVERSAMPLING=FALSE,USE_UNDERSAMPLING=TRUE\n",
    "                                          ,sample_thred=0.5)\n",
    "\n",
    "my.xgb2 = XGBoostfit(train_sample,seed,FALSE)\n",
    "\n",
    "\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "threas_XGB2=0.8\n",
    "xgb_results2=eval(test_episodes_list,my.xgb2,threas_XGB,USE_XGB=TRUE,N=N,moving_step = moving_step,days=days)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine algorithm predictions\n",
    "\n",
    "**1) function: return_preds**\n",
    "\n",
    "**2) function: combine_algos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that returns the algos_predictions.\n",
    "#     2 types of predictions:\n",
    "#   1)If regression_ret=FALSE the predictions for each algo are TRUE-FALSE(0-1), with the help of a threshold. \n",
    "#   2)If regression_ret=TRUE the predictions for each algo are decimal numbers.\n",
    "return_preds<-function(test_episodes_list,fitted_model,USE_NN=FALSE,USE_LSTM=FALSE,USE_CNN=FALSE\n",
    "                       ,USE_KNN=FALSE,USE_PLS=FALSE,threas=0.5,regression_ret=FALSE,number_of_timestamps=1){\n",
    "  \n",
    "  return_Predictions=list()\n",
    "  count=0\n",
    "  knn_flag=0\n",
    "  \n",
    "  for(ep in test_episodes_list){\n",
    "    \n",
    "    ep = ep[ , !(names(ep) %in% c(\"Timestamps\"))]\n",
    "    ep = ep[ , !(names(ep) %in% c(\"Risk_F\"))]\n",
    "    test_set = data.matrix(ep[,1:length(ep[1,])])\n",
    "    \n",
    "    if(USE_LSTM||USE_CNN){\n",
    "      test_set = array(test_set, dim=c(length(ep[,1]),number_of_timestamps,length(ep[1,])/number_of_timestamps))  \n",
    "    }else{\n",
    "      test_set = array(test_set, dim=c(length(ep[,1]),length(ep[1,])))\n",
    "    }\n",
    "    \n",
    "    if(USE_LSTM||USE_CNN||USE_NN){\n",
    "      Prediction = fitted_model %>% predict(test_set) #make the prediction of the episode\n",
    "      if(!regression_ret){\n",
    "        Prediction=Prediction>threas\n",
    "      }\n",
    "      \n",
    "    }else if(USE_KNN){\n",
    "      \n",
    "      if(knn_flag==0){\n",
    "        before_ep=1\n",
    "        Prediction=fitted_model[before_ep:(before_ep+length(ep[,1])-1)]\n",
    "        knn_flag=10\n",
    "      }\n",
    "      else{\n",
    "        Prediction=fitted_model[before_ep:(before_ep+length(ep[,1])-1)]\n",
    "      }\n",
    "      before_ep=before_ep+length(ep[,1])\n",
    "      if(!regression_ret){\n",
    "        Prediction = (Prediction >= threas)==TRUE\n",
    "      }\n",
    "      \n",
    "      #pred_indeces = as.numeric(which((Prediction >= threas)==TRUE))\n",
    "      \n",
    "    }else if(USE_PLS){\n",
    "        Prediction=(data.matrix(as.matrix(ep))) %*% data.matrix(fitted_model$reg.coefs[-1])\n",
    "        Prediction[,1]=Prediction[,1]+as.numeric(fitted_model$reg.coefs[1])\n",
    "        if(!regression_ret){\n",
    "            Prediction = ((Prediction >= acceptance_threshold)==TRUE) #acceptance_threshold=0.5  \n",
    "        }    \n",
    "    }\n",
    "    else{\n",
    "      Prediction <- predict(fitted_model, as.matrix(ep)) #make the prediction of the episode\n",
    "      if(!regression_ret){\n",
    "        Prediction=Prediction>threas\n",
    "      }\n",
    "\n",
    "    }\n",
    "    \n",
    "    count=count+1\n",
    "    return_Predictions[count]=list(Prediction)\n",
    "    \n",
    "  }\n",
    "  \n",
    "  return(return_Predictions)\n",
    "  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function that combines the algos_predictions.\n",
    "#     2 types of predictions:\n",
    "#   1)If reg_pred=FALSE the predictions for each algo are TRUE-FALSE(0-1), with the help of a threshold. \n",
    "#   2)If reg_pred=TRUE the predictions for each algo are decimal numbers.\n",
    "\n",
    "#If return_preds=FALSE the function print the results, else returns the combined predictions.\n",
    "\n",
    "combine_algos<-function(algos_predictions,percent=0.5,return_preds=FALSE\n",
    "                        ,seed=0,reg_pred=FALSE,reg_threas=0.5,return_reg=FALSE\n",
    "                       ,N,moving_step,days,weights=list()){\n",
    "  \n",
    " sum_of_algos=list()\n",
    "  \n",
    "  num_of_episodes=length(algos_predictions[[1]])  \n",
    "\n",
    "  for(i in 1:num_of_episodes){\n",
    "    \n",
    "    sum_of_algos[i]=list(array(0,dim=c(length(algos_predictions[[1]][[i]]))))\n",
    "    count_alg=0\n",
    "    for(algo in algos_predictions){\n",
    "      count_alg=count_alg+1  \n",
    "      pred_ep=(algo)[[i]]\n",
    "        \n",
    "      for(j in 1:length(pred_ep)){\n",
    "        if(reg_pred){\n",
    "          if(length(weights)==0){   \n",
    "              sum_of_algos[[i]][[j]]=sum_of_algos[[i]][[j]]+pred_ep[[j]]\n",
    "          }else{\n",
    "              sum_of_algos[[i]][[j]]=sum_of_algos[[i]][[j]]+pred_ep[[j]]*as.numeric(weights[[count_alg]])\n",
    "          }\n",
    "        }else{\n",
    "          if(pred_ep[[j]]==TRUE){\n",
    "               \n",
    "            if(length(weights)==0){ \n",
    "                sum_of_algos[[i]][[j]]=sum_of_algos[[i]][[j]]+1\n",
    "            }else{\n",
    "                sum_of_algos[[i]][[j]]=sum_of_algos[[i]][[j]]+as.numeric(weights[[count_alg]])\n",
    "            }  \n",
    "              \n",
    "              \n",
    "          }\n",
    "          \n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  \n",
    "\n",
    "  if(reg_pred){\n",
    "    if(return_reg&&return_preds){\n",
    "        if(length(weights)==0){ \n",
    "            for(i in 1:num_of_episodes){\n",
    "              sum_of_algos[[i]]=sum_of_algos[[i]]/length(algos_predictions)\n",
    "            }\n",
    "        }\n",
    "    }else{\n",
    "        if(length(weights)==0){ \n",
    "            for(i in 1:num_of_episodes){\n",
    "              sum_of_algos[[i]]=(sum_of_algos[[i]]/length(algos_predictions))>=reg_threas\n",
    "            }\n",
    "        }else{\n",
    "            for(i in 1:num_of_episodes){\n",
    "              sum_of_algos[[i]]=(sum_of_algos[[i]])>=reg_threas\n",
    "            }\n",
    "        }\n",
    "    }    \n",
    "  }else{\n",
    "    for(i in 1:num_of_episodes){\n",
    "      if(length(weights)==0){   \n",
    "          sum_of_algos[[i]]=sum_of_algos[[i]]>=ceiling(length(algos_predictions)*percent)\n",
    "      }else{\n",
    "          sum_of_algos[[i]]=sum_of_algos[[i]]>=percent   \n",
    "      }  \n",
    "      \n",
    "    }\n",
    "  }\n",
    "  \n",
    "  \n",
    "  if(return_preds){\n",
    "    return(sum_of_algos)\n",
    "  }\n",
    "  \n",
    "  eval_model(seed=seed,predictions=sum_of_algos,N=N,moving_step = moving_step,days=days)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the bool-predictions  of each algorithm and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF results:\n",
      "-------------------------------------------------------------\n",
      "dataset: C:/Users/petsi/Documents/ptyxiakh/testing_my_dataset_150.csv \n",
      "true_positives: 14 \n",
      "false_positives: 10 \n",
      "false_negatives: 8 \n",
      "precision: 0.583333333333333 \n",
      "recall: 0.636363636363636 \n",
      "F1: 0.608695652173913 \n",
      "[1] \"F1 score is:\"\n",
      "[1] 0.6086957\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.608695652173913"
      ],
      "text/latex": [
       "0.608695652173913"
      ],
      "text/markdown": [
       "0.608695652173913"
      ],
      "text/plain": [
       "[1] 0.6086957"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "\n",
      "XGB results:\n",
      "-------------------------------------------------------------\n",
      "dataset: C:/Users/petsi/Documents/ptyxiakh/testing_my_dataset_150.csv \n",
      "true_positives: 18 \n",
      "false_positives: 13 \n",
      "false_negatives: 4 \n",
      "precision: 0.580645161290323 \n",
      "recall: 0.818181818181818 \n",
      "F1: 0.679245283018868 \n",
      "[1] \"F1 score is:\"\n",
      "[1] 0.6792453\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.679245283018868"
      ],
      "text/latex": [
       "0.679245283018868"
      ],
      "text/markdown": [
       "0.679245283018868"
      ],
      "text/plain": [
       "[1] 0.6792453"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "\n",
      "PLS results:\n",
      "-------------------------------------------------------------\n",
      "dataset: C:/Users/petsi/Documents/ptyxiakh/testing_my_dataset_150.csv \n",
      "true_positives: 8 \n",
      "false_positives: 7 \n",
      "false_negatives: 14 \n",
      "precision: 0.533333333333333 \n",
      "recall: 0.363636363636364 \n",
      "F1: 0.432432432432432 \n",
      "[1] \"F1 score is:\"\n",
      "[1] 0.4324324\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.432432432432432"
      ],
      "text/latex": [
       "0.432432432432432"
      ],
      "text/markdown": [
       "0.432432432432432"
      ],
      "text/plain": [
       "[1] 0.4324324"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "\n",
      "KNN results:\n",
      "-------------------------------------------------------------\n",
      "dataset: C:/Users/petsi/Documents/ptyxiakh/testing_my_dataset_150.csv \n",
      "true_positives: 17 \n",
      "false_positives: 22 \n",
      "false_negatives: 5 \n",
      "precision: 0.435897435897436 \n",
      "recall: 0.772727272727273 \n",
      "F1: 0.557377049180328 \n",
      "[1] \"F1 score is:\"\n",
      "[1] 0.557377\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.557377049180328"
      ],
      "text/latex": [
       "0.557377049180328"
      ],
      "text/markdown": [
       "0.557377049180328"
      ],
      "text/plain": [
       "[1] 0.557377"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "\n",
      "NN results:\n",
      "-------------------------------------------------------------\n",
      "dataset: C:/Users/petsi/Documents/ptyxiakh/testing_my_dataset_150.csv \n",
      "true_positives: 15 \n",
      "false_positives: 12 \n",
      "false_negatives: 7 \n",
      "precision: 0.555555555555556 \n",
      "recall: 0.681818181818182 \n",
      "F1: 0.612244897959184 \n",
      "[1] \"F1 score is:\"\n",
      "[1] 0.6122449\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.612244897959184"
      ],
      "text/latex": [
       "0.612244897959184"
      ],
      "text/markdown": [
       "0.612244897959184"
      ],
      "text/plain": [
       "[1] 0.6122449"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "\n",
      "SAMPLE results:\n",
      "-------------------------------------------------------------\n",
      "dataset: C:/Users/petsi/Documents/ptyxiakh/testing_my_dataset_150.csv \n",
      "true_positives: 7 \n",
      "false_positives: 9 \n",
      "false_negatives: 15 \n",
      "precision: 0.4375 \n",
      "recall: 0.318181818181818 \n",
      "F1: 0.368421052631579 \n",
      "[1] \"F1 score is:\"\n",
      "[1] 0.3684211\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.368421052631579"
      ],
      "text/latex": [
       "0.368421052631579"
      ],
      "text/markdown": [
       "0.368421052631579"
      ],
      "text/plain": [
       "[1] 0.3684211"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cat(\"RF results:\\n\")\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "bool_preds_RF=return_preds(test_episodes_list_rf,my.rf,threas=threas_RF)\n",
    "eval_model(seed=seed,predictions=bool_preds_RF,N=N,moving_step = moving_step,days=days)\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "\n",
    "cat(\"\\nXGB results:\\n\")\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "bool_preds_XGB=return_preds(test_episodes_list_XGB,my.xgb,threas=threas_XGB)\n",
    "eval_model(seed=seed,predictions=bool_preds_XGB,N=N,moving_step = moving_step,days=days)\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "\n",
    "cat(\"\\nPLS results:\\n\")\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "bool_preds_PLS=return_preds(test_episodes_list_PLS,my.pls,USE_PLS=TRUE,threas=threas_PLS)\n",
    "eval_model(seed=seed,predictions=bool_preds_PLS,N=N,moving_step = moving_step,days=days)\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "\n",
    "cat(\"\\nKNN results:\\n\")\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "bool_preds_KNN=return_preds(test_episodes_list_KNN,prediction_knn_fast,USE_KNN=TRUE,threas=threas_KNN)\n",
    "eval_model(seed=seed,predictions=bool_preds_KNN,N=N,moving_step = moving_step,days=days)\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "\n",
    "cat(\"\\nNN results:\\n\")\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "bool_preds_NN=return_preds(test_episodes_list_model,modeling,USE_NN=TRUE,USE_LSTM=USE_LSTM,USE_CNN=USE_CNN\n",
    "                           ,threas=threas_MODEL,number_of_timestamps=inputs_shape[1])\n",
    "eval_model(seed=seed,predictions=bool_preds_NN,N=N,moving_step = moving_step,days=days)\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "\n",
    "cat(\"\\nSAMPLE results:\\n\")\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "bool_preds_XGB2=return_preds(test_episodes_list,my.xgb2,threas=threas_XGB2)\n",
    "eval_model(seed=seed,predictions=bool_preds_XGB2,N=N,moving_step = moving_step,days=days)\n",
    "cat(\"-------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a list of the algorithms' bool-predictions you want to combine and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF, KNN and PLS combination results:\n",
      "-------------------------------------------------------------\n",
      "dataset: C:/Users/petsi/Documents/ptyxiakh/testing_my_dataset_150.csv \n",
      "true_positives: 12 \n",
      "false_positives: 9 \n",
      "false_negatives: 10 \n",
      "precision: 0.571428571428571 \n",
      "recall: 0.545454545454545 \n",
      "F1: 0.558139534883721 \n",
      "[1] \"F1 score is:\"\n",
      "[1] 0.5581395\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.558139534883721"
      ],
      "text/latex": [
       "0.558139534883721"
      ],
      "text/markdown": [
       "0.558139534883721"
      ],
      "text/plain": [
       "[1] 0.5581395"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "\n",
      "RF-KNN-PLS and XGBoost combination results:\n",
      "-------------------------------------------------------------\n",
      "dataset: C:/Users/petsi/Documents/ptyxiakh/testing_my_dataset_150.csv \n",
      "true_positives: 19 \n",
      "false_positives: 17 \n",
      "false_negatives: 3 \n",
      "precision: 0.527777777777778 \n",
      "recall: 0.863636363636364 \n",
      "F1: 0.655172413793103 \n",
      "[1] \"F1 score is:\"\n",
      "[1] 0.6551724\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.655172413793103"
      ],
      "text/latex": [
       "0.655172413793103"
      ],
      "text/markdown": [
       "0.655172413793103"
      ],
      "text/plain": [
       "[1] 0.6551724"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset: C:/Users/petsi/Documents/ptyxiakh/testing_my_dataset_150.csv \n",
      "true_positives: 18 \n",
      "false_positives: 13 \n",
      "false_negatives: 4 \n",
      "precision: 0.580645161290323 \n",
      "recall: 0.818181818181818 \n",
      "F1: 0.679245283018868 \n",
      "[1] \"F1 score is:\"\n",
      "[1] 0.6792453\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.679245283018868"
      ],
      "text/latex": [
       "0.679245283018868"
      ],
      "text/markdown": [
       "0.679245283018868"
      ],
      "text/plain": [
       "[1] 0.6792453"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "algos_predictions=list(bool_preds_RF,bool_preds_KNN,bool_preds_PLS)\n",
    "\n",
    "mid=combine_algos(algos_predictions,percent=2/3,seed=seed,return_preds=TRUE)\n",
    "cat(\"RF, KNN and PLS combination results:\\n\")\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "eval_model(seed=seed,predictions=mid,N=N,moving_step = moving_step,days=days)\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "cat(\"\\nRF-KNN-PLS and XGBoost combination results:\\n\")\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "algos_predictions=list(mid,bool_preds_XGB)\n",
    "combine_algos(algos_predictions,percent=1/2,seed=seed,N=N,moving_step = moving_step,days=days)\n",
    "weight=list(0.4,0.6)\n",
    "combine_algos(algos_predictions,percent=1/2,seed=seed,N=N,moving_step = moving_step,days=days,weight=weight)\n",
    "cat(\"-------------------------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect the reg-predictions  of each algorithm and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_preds_RF=return_preds(test_episodes_list_rf,my.rf,threas=threas_RF,regression_ret=TRUE)\n",
    "reg_preds_XGB=return_preds(test_episodes_list_XGB,my.xgb,threas=threas_XGB,regression_ret=TRUE)\n",
    "reg_preds_PLS=return_preds(test_episodes_list_PLS,my.pls,USE_PLS=TRUE,threas=threas_PLS,regression_ret=TRUE)\n",
    "reg_preds_KNN=return_preds(test_episodes_list_KNN,prediction_knn_fast,USE_KNN=TRUE,threas=threas_KNN,regression_ret=TRUE)\n",
    "reg_preds_NN=return_preds(test_episodes_list_model,modeling,USE_NN=TRUE,,USE_LSTM=USE_LSTM,USE_CNN=USE_CNN\n",
    "                          ,threas=threas_MODEL,regression_ret=TRUE,number_of_timestamps=inputs_shape[1])\n",
    "reg_preds_SAMPLE=return_preds(test_episodes_list,my.xgb2,threas=threas_XGB2,regression_ret=TRUE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a list of the algorithms' reg-predictions you want to combine and print the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN and RF combination results:\n",
      "-------------------------------------------------------------\n",
      "dataset: C:/Users/petsi/Documents/ptyxiakh/testing_my_dataset_150.csv \n",
      "true_positives: 14 \n",
      "false_positives: 10 \n",
      "false_negatives: 8 \n",
      "precision: 0.583333333333333 \n",
      "recall: 0.636363636363636 \n",
      "F1: 0.608695652173913 \n",
      "[1] \"F1 score is:\"\n",
      "[1] 0.6086957\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.608695652173913"
      ],
      "text/latex": [
       "0.608695652173913"
      ],
      "text/markdown": [
       "0.608695652173913"
      ],
      "text/plain": [
       "[1] 0.6086957"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "RF and XGB combination results:\n",
      "-------------------------------------------------------------\n",
      "dataset: C:/Users/petsi/Documents/ptyxiakh/testing_my_dataset_150.csv \n",
      "true_positives: 18 \n",
      "false_positives: 13 \n",
      "false_negatives: 4 \n",
      "precision: 0.580645161290323 \n",
      "recall: 0.818181818181818 \n",
      "F1: 0.679245283018868 \n",
      "[1] \"F1 score is:\"\n",
      "[1] 0.6792453\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.679245283018868"
      ],
      "text/latex": [
       "0.679245283018868"
      ],
      "text/markdown": [
       "0.679245283018868"
      ],
      "text/plain": [
       "[1] 0.6792453"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "\n",
      "KNN-RF, RF-XGBoost and PLS and RF combination results:\n",
      "-------------------------------------------------------------\n",
      "dataset: C:/Users/petsi/Documents/ptyxiakh/testing_my_dataset_150.csv \n",
      "true_positives: 18 \n",
      "false_positives: 11 \n",
      "false_negatives: 4 \n",
      "precision: 0.620689655172414 \n",
      "recall: 0.818181818181818 \n",
      "F1: 0.705882352941177 \n",
      "[1] \"F1 score is:\"\n",
      "[1] 0.7058824\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.705882352941177"
      ],
      "text/latex": [
       "0.705882352941177"
      ],
      "text/markdown": [
       "0.705882352941177"
      ],
      "text/plain": [
       "[1] 0.7058824"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------\n",
      "dataset: C:/Users/petsi/Documents/ptyxiakh/testing_my_dataset_150.csv \n",
      "true_positives: 18 \n",
      "false_positives: 11 \n",
      "false_negatives: 4 \n",
      "precision: 0.620689655172414 \n",
      "recall: 0.818181818181818 \n",
      "F1: 0.705882352941177 \n",
      "[1] \"F1 score is:\"\n",
      "[1] 0.7058824\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "0.705882352941177"
      ],
      "text/latex": [
       "0.705882352941177"
      ],
      "text/markdown": [
       "0.705882352941177"
      ],
      "text/plain": [
       "[1] 0.7058824"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "algos_predictions=list(reg_preds_KNN,bool_preds_RF)\n",
    "reg_threas=0.4\n",
    "reg_mid1=combine_algos(algos_predictions,seed=seed,return_preds=TRUE,reg_pred=TRUE\n",
    "                      ,reg_threas=reg_threas,return_reg=FALSE,N=N,moving_step = moving_step,days=days)\n",
    "cat(\"KNN and RF combination results:\\n\")\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "eval_model(seed=seed,predictions=reg_mid1,N=N,moving_step = moving_step,days=days)\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "\n",
    "algos_predictions=list(reg_preds_RF,bool_preds_XGB)\n",
    "reg_threas=0.4\n",
    "reg_mid2=combine_algos(algos_predictions,seed=seed,return_preds=TRUE,reg_pred=TRUE\n",
    "                      ,reg_threas=reg_threas,return_reg=FALSE,N=N,moving_step = moving_step,days=days)\n",
    "cat(\"RF and XGB combination results:\\n\")\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "eval_model(seed=seed,predictions=reg_mid2,N=N,moving_step = moving_step,days=days)\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "\n",
    "\n",
    "cat(\"\\nKNN-RF, RF-XGBoost and PLS and RF combination results:\\n\")\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "algos_predictions=list(reg_mid1,reg_mid2,reg_preds_PLS)\n",
    "combine_algos(algos_predictions,reg_threas=0.4,seed=seed,reg_pred=TRUE,N=N,moving_step = moving_step,days=days)\n",
    "cat(\"-------------------------------------------------------------\\n\")\n",
    "combine_algos(algos_predictions,reg_threas=0.4,seed=seed,reg_pred=TRUE,N=N,moving_step = moving_step,days=days\n",
    "              ,weights=list(0.3,0.3,0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
